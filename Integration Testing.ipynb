{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1521bc8c-0ae5-4352-aa9e-52b2fbc6016d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /databricks/python3/lib/python3.11/site-packages (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests) (2023.7.22)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46243ce7-ae0d-4da2-9160-9c617f87cdb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def extract_products():\n",
    "    \"\"\"\n",
    "    Extract 50 users and 100 products from FakerAPI.\n",
    "    \"\"\"\n",
    "    product_url = \"https://fakerapi.it/api/v2/products?_quantity=100\"\n",
    "    \n",
    "    print(\"Fetching products from API...\")\n",
    "    product_response = requests.get(product_url)\n",
    "    product_response.raise_for_status()\n",
    "    \n",
    "    data = product_response.json()[\"data\"]\n",
    "    products_data = []\n",
    "\n",
    "    for product in data:\n",
    "      products_data.append({\n",
    "          \"id\": product[\"id\"],\n",
    "          \"name\": product[\"name\"],\n",
    "          \"description\": product[\"description\"],\n",
    "          \"upc\": product[\"upc\"],\n",
    "          \"image\": product[\"image\"],\n",
    "          \"net_price\": product[\"net_price\"],\n",
    "          \"taxes\": product[\"taxes\"],\n",
    "          \"price\": product[\"price\"],\n",
    "          \"categories\": product[\"categories\"]\n",
    "      })\n",
    "    \n",
    "    return products_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82ef953e-30e3-4021-931c-452ca1bcc87c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_extraction():\n",
    "    \"\"\"\n",
    "    Unit test to validate schema and data types from user and product APIs.\n",
    "    \"\"\"\n",
    "    print(\"=== UNIT TEST: extract_users_and_products ===\")\n",
    "    \n",
    "    products = extract_products()\n",
    "    \n",
    "    for product in products:\n",
    "        assert \"id\" in product and isinstance(product[\"id\"], int)\n",
    "        assert \"name\" in product and isinstance(product[\"name\"], str)\n",
    "        assert \"description\" in product and isinstance(product[\"description\"], str)\n",
    "        assert \"upc\" in product and isinstance(product[\"upc\"], str)\n",
    "        assert \"image\" in product and isinstance(product[\"image\"], str)\n",
    "        assert \"net_price\" in product and isinstance(product[\"net_price\"], (int, float))\n",
    "        assert \"taxes\" in product and isinstance(product[\"taxes\"], (int, float))\n",
    "        assert \"price\" in product and isinstance(product[\"price\"], (int, float))\n",
    "        assert \"categories\" in product and isinstance(product[\"categories\"], list)\n",
    "\n",
    "    print(\"✅ All schema and data type tests PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37f87755-bbb6-41e6-b929-5469a8ac1e55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== UNIT TEST: extract_users_and_products ===\nFetching products from API...\n✅ All schema and data type tests PASSED\n"
     ]
    }
   ],
   "source": [
    "test_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8365a8d5-d5a4-4290-9e03-f0b1fadadd60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform_product_data(raw_products):\n",
    "    \"\"\"\n",
    "    Transform API product data to our internal format.\n",
    "    Skips invalid or corrupt records.\n",
    "    \"\"\"\n",
    "    transformed_products = []\n",
    "    for product in raw_products:\n",
    "        try:\n",
    "            if not product.get('id') and isinstance(product.get('id'), int):\n",
    "              print(f\"Skipping product ID {product['id']}: Missing supplier_id\")\n",
    "              continue\n",
    "\n",
    "            price = float(product['price'])\n",
    "            net_price = float(product.get('net_price', 0))\n",
    "            if price <= 0:\n",
    "                print(f\"Skipping product ID {product['id']}: Non-positive price ({price})\")\n",
    "                continue\n",
    "\n",
    "            clean_product = {\n",
    "                'product_id': product['id'],\n",
    "                'price': price,\n",
    "                'net_price': net_price,\n",
    "                'description': product['description'],\n",
    "                'taxes': product.get('taxes')\n",
    "            }\n",
    "            transformed_products.append(clean_product)\n",
    "\n",
    "        except (ValueError, TypeError) as e:\n",
    "            print(f\"Skipping product ID {product.get('id', 'Unknown')}: Invalid price format → {e}\")\n",
    "            continue\n",
    "\n",
    "    return transformed_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e5378aa-8e9f-447e-88c6-999212cdf26d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_transform_product_data():\n",
    "    print(\"\\n=== UNIT TEST: transform_product_data ===\")\n",
    "\n",
    "    raw_data = [\n",
    "      # ✅ Valid product\n",
    "      {\n",
    "          \"id\": 1,\n",
    "          \"name\": \"Valid Product 1\",\n",
    "          \"description\": \"All values present and valid.\",\n",
    "          \"ean\": \"1234567890123\",\n",
    "          \"upc\": \"123456789012\",\n",
    "          \"price\": 199.99,\n",
    "          \"net_price\": 179.99,\n",
    "          \"taxes\": 25\n",
    "      },\n",
    "\n",
    "      # ❌ Negative price\n",
    "      {\n",
    "          \"id\": 2,\n",
    "          \"name\": \"Negative Price Product\",\n",
    "          \"description\": \"Price is negative.\",\n",
    "          \"ean\": \"1234567890125\",\n",
    "          \"upc\": \"123456789014\",\n",
    "          \"price\": -89.99,\n",
    "          \"net_price\": -79.99,\n",
    "          \"taxes\": 25\n",
    "      },\n",
    "\n",
    "      # ❌ Zero price\n",
    "      {\n",
    "          \"id\": 3,\n",
    "          \"name\": \"Zero Price Product\",\n",
    "          \"description\": \"Zero price should be skipped.\",\n",
    "          \"ean\": \"1234567890126\",\n",
    "          \"upc\": \"123456789015\",\n",
    "          \"price\": 0.0,\n",
    "          \"net_price\": 0.0,\n",
    "          \"taxes\": 25\n",
    "      },\n",
    "\n",
    "      # ✅ Another valid product\n",
    "      {\n",
    "          \"id\": 4,\n",
    "          \"name\": \"Valid Product 2\",\n",
    "          \"description\": \"Everything is clean.\",\n",
    "          \"ean\": \"1234567890127\",\n",
    "          \"upc\": \"123456789016\",\n",
    "          \"price\": 59.5,\n",
    "          \"net_price\": 50.0,\n",
    "          \"taxes\": 25\n",
    "      },\n",
    "\n",
    "      # ❌ Price as a string (invalid type)\n",
    "      {\n",
    "          \"id\": 5,\n",
    "          \"name\": \"Invalid Type Product\",\n",
    "          \"description\": \"Price is a string.\",\n",
    "          \"ean\": \"1234567890128\",\n",
    "          \"upc\": \"123456789017\",\n",
    "          \"price\": \"free\",\n",
    "          \"net_price\": \"zero\",\n",
    "          \"taxes\": 25\n",
    "      },\n",
    "\n",
    "      # ✅ Valid product\n",
    "      {\n",
    "          \"id\": 8,\n",
    "          \"name\": \"Valid Product 3\",\n",
    "          \"description\": \"Great product with full data.\",\n",
    "          \"ean\": \"1234567890130\",\n",
    "          \"upc\": \"123456789019\",\n",
    "          \"price\": 300.0,\n",
    "          \"net_price\": 280.0,\n",
    "          \"taxes\": 25\n",
    "      }\n",
    "  ]\n",
    "\n",
    "\n",
    "    expected_ids = [1, 4, 8]  # Only these should be transformed successfully\n",
    "\n",
    "    result = transform_product_data(raw_data)\n",
    "    result_ids = [r[\"product_id\"] for r in result]\n",
    "\n",
    "    assert result_ids == expected_ids, f\"❌ Transform test FAILED. Expected: {expected_ids}, Got: {result_ids}\"\n",
    "    print(\"✅ Transform test PASSED\")\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd5d0cb2-0e46-4743-a4d8-77bf878e2849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== UNIT TEST: transform_product_data ===\nSkipping product ID 2: Non-positive price (-89.99)\nSkipping product ID 3: Non-positive price (0.0)\nSkipping product ID 5: Invalid price format → could not convert string to float: 'free'\n✅ Transform test PASSED\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_transform_product_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4170427b-5fed-4530-8670-7cca2f80bbda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def simulate_problematic_api_data(real_api_data):\n",
    "    \"\"\"\n",
    "    Simulate edge cases that might occur in real API responses.\n",
    "    These represent scenarios not covered in unit testing.\n",
    "    \"\"\"\n",
    "    # Start with a copy of real data\n",
    "    problematic_data = real_api_data.copy()\n",
    "    \n",
    "    # Edge Case 1: Completely corrupted product structure\n",
    "    problematic_data.append({\n",
    "        \"id\": None,  # Null ID\n",
    "        \"name\": \"\",  # Empty name\n",
    "        \"description\": None,  # Null description\n",
    "        \"categories\": \"not_a_list\",  # Wrong type\n",
    "        # Missing all price-related fields\n",
    "    })\n",
    "    \n",
    "    # Edge Case 2: Product with nested price structure (API change)\n",
    "    problematic_data.append({\n",
    "        \"id\": 99999,\n",
    "        \"name\": \"Nested Price Product\",\n",
    "        \"description\": \"Product with complex pricing\",\n",
    "        \"price\": {  # Price is now an object instead of number\n",
    "            \"amount\": 150.0,\n",
    "            \"currency\": \"USD\"\n",
    "        },\n",
    "        \"net_price\": [129.99],  # Net price is now a list\n",
    "    })\n",
    "    \n",
    "    # Edge Case 3: Product with extreme data values\n",
    "    problematic_data.append({\n",
    "        \"id\": \"not_a_number\",  # ID as string instead of int\n",
    "        \"name\": \"A\" * 1000,  # Extremely long name\n",
    "        \"description\": None,\n",
    "        \"price\": float('inf'),  # Infinite price\n",
    "        \"net_price\": float('-inf'),  # Negative infinite net price\n",
    "        \"taxes\": 20\n",
    "    })\n",
    "    \n",
    "    # Edge Case 4: Product with special characters causing encoding issues\n",
    "    problematic_data.append({\n",
    "        \"id\": None,\n",
    "        \"name\": \"Product with 特殊字符 and émojis 🚀💰\",\n",
    "        \"description\": \"Test\\x00null\\x00byte\",  # Null bytes in string\n",
    "        \"price\": \"€199.99\",  # Price with currency symbol\n",
    "        \"net_price\": \"£150,50\",  # Price with comma decimal separator\n",
    "        \"taxes\": 10\n",
    "    })\n",
    "    \n",
    "    return problematic_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c457dabe-525c-44d7-b720-6b3f6f1b0bbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_integration_extract_and_transform():\n",
    "    \"\"\"\n",
    "    Integration test that combines extract_products() and transform_product_data().\n",
    "    This test is designed to FAIL due to realistic edge cases not handled by the functions.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🧪 INTEGRATION TEST: extract_products + transform_product_data\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Extract real data from API\n",
    "        print(\"Step 1: Extracting data from API...\")\n",
    "        real_api_data = extract_products()\n",
    "        print(f\"Successfully extracted {len(real_api_data)} products from API\")\n",
    "        \n",
    "        # Step 2: Simulate problematic data that might occur in production\n",
    "        print(\"\\nStep 2: Simulating edge cases not covered in unit testing...\")\n",
    "        problematic_data = simulate_problematic_api_data(real_api_data)\n",
    "        print(f\"Created {len(problematic_data)} products with edge cases\")\n",
    "        \n",
    "        # Step 3: Attempt transformation (this should reveal failures)\n",
    "        print(\"\\nStep 3: Attempting to transform problematic data...\")\n",
    "        transformed_data = transform_product_data(problematic_data)\n",
    "        \n",
    "        # Test whether length of data taken as an input is equal to the length of output data\n",
    "        assert len(problematic_data) == len(transformed_data), \"❌ Data is skipped due to None values in product_id, or due to other reason!\"\n",
    "        \n",
    "        # This assertion will FAIL - we expect all products to have required fields\n",
    "        for product in transformed_data:\n",
    "            assert 'product_id' in product, \"Missing product_id in transformed data\"\n",
    "            assert 'price' in product, \"Missing price in transformed data\" \n",
    "            assert product['price'] > 0, \"Invalid price in transformed data\"\n",
    "        \n",
    "        print(\"✅ Integration test PASSED (unexpected)\")\n",
    "        return True\n",
    "        \n",
    "    except AssertionError as e:\n",
    "        print(f\"\\nINTEGRATION TEST FAILED: {e}\")\n",
    "        print(\"\\n🔍 Analysis of failures:\")\n",
    "        print(\"- Missing 'supplier_id' field in API response (not provided by FakerAPI)\")\n",
    "        print(\"- Missing 'price' field in some products (simulated API inconsistency)\")\n",
    "        print(\"- Corrupted data structures not handled by transform function\")\n",
    "        print(\"- Special data types and edge values causing transformation failures\")\n",
    "        print(\"\\n💡 These failures represent realistic production scenarios!\")\n",
    "        return False\n",
    "        \n",
    "    # except Exception as e:\n",
    "    #     print(f\"\\nINTEGRATION TEST CRASHED: {e}\")\n",
    "    #     print(\"This indicates the functions cannot handle unexpected data formats\")\n",
    "    #     return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74886312-7ff2-4240-b4b1-3aa250c7f486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n============================================================\n🧪 INTEGRATION TEST: extract_products + transform_product_data\n============================================================\nStep 1: Extracting data from API...\nFetching products from API...\nSuccessfully extracted 100 products from API\n\nStep 2: Simulating edge cases not covered in unit testing...\nCreated 104 products with edge cases\n\nStep 3: Attempting to transform problematic data...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8085892093823539>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Extract + Transform with edge cases\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m test_integration_extract_and_transform()\n",
       "\n",
       "File \u001B[0;32m<command-7829083480830920>, line 23\u001B[0m, in \u001B[0;36mtest_integration_extract_and_transform\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Step 3: Attempt transformation (this should reveal failures)\u001B[39;00m\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mStep 3: Attempting to transform problematic data...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m---> 23\u001B[0m transformed_data \u001B[38;5;241m=\u001B[39m transform_product_data(problematic_data)\n",
       "\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# Test whether length of data taken as an input is equal to the length of output data\u001B[39;00m\n",
       "\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(problematic_data) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(transformed_data), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m❌ Data is skipped due to None values in product_id, or due to other reason!\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\n",
       "File \u001B[0;32m<command-8085892093823540>, line 13\u001B[0m, in \u001B[0;36mtransform_product_data\u001B[0;34m(raw_products)\u001B[0m\n",
       "\u001B[1;32m     10\u001B[0m   \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSkipping product ID \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mproduct[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: Missing supplier_id\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     11\u001B[0m   \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
       "\u001B[0;32m---> 13\u001B[0m price \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(product[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprice\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
       "\u001B[1;32m     14\u001B[0m net_price \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(product\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnet_price\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m0\u001B[39m))\n",
       "\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m price \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m: 'price'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "KeyError",
        "evalue": "'price'"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>KeyError</span>: 'price'"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
        "File \u001B[0;32m<command-8085892093823539>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Extract + Transform with edge cases\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m test_integration_extract_and_transform()\n",
        "File \u001B[0;32m<command-7829083480830920>, line 23\u001B[0m, in \u001B[0;36mtest_integration_extract_and_transform\u001B[0;34m()\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Step 3: Attempt transformation (this should reveal failures)\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mStep 3: Attempting to transform problematic data...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 23\u001B[0m transformed_data \u001B[38;5;241m=\u001B[39m transform_product_data(problematic_data)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# Test whether length of data taken as an input is equal to the length of output data\u001B[39;00m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(problematic_data) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(transformed_data), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m❌ Data is skipped due to None values in product_id, or due to other reason!\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
        "File \u001B[0;32m<command-8085892093823540>, line 13\u001B[0m, in \u001B[0;36mtransform_product_data\u001B[0;34m(raw_products)\u001B[0m\n\u001B[1;32m     10\u001B[0m   \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSkipping product ID \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mproduct[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: Missing supplier_id\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     11\u001B[0m   \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m price \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(product[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprice\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     14\u001B[0m net_price \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(product\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnet_price\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m0\u001B[39m))\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m price \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
        "\u001B[0;31mKeyError\u001B[0m: 'price'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract + Transform with edge cases\n",
    "test_integration_extract_and_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fcf4c61-141a-4993-99eb-ca547df404f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "The above error clearly states that we forgot to deal with the case that what will happen if price is empty, the price doesn't exists, and that's why there is no logic to handle this situation, so from our end we were sure that records will contain price, but in realistic some situation can come where the column that we expect doesn't exists and in that case it can leads to failures or big impact, and thus to identify that issue for reporting, we perform integration testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afdfd670-4efa-4e4f-bdc4-1176e2d014ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### First start with the below simple use case, and then proceed with the above use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "472c32b2-ee9c-40d1-823b-5b2be2fa6520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_source_data():\n",
    "    \"\"\"Simulates extracting raw data.\"\"\"\n",
    "    print(\"EXTRACT: Getting raw customer data...\")\n",
    "\n",
    "    # Assume that this data is fetched from the database\n",
    "    raw_data = [\n",
    "        {\"id\": 1, \"name\": \"Alice Wonderland\", \"email\": \"alice@example.com\"},\n",
    "        {\"id\": 2, \"name\": \"\", \"email\": \"bob@example.com\"},  # missing name\n",
    "        {\"id\": 3, \"name\": \"Charlie Brown\", \"email\": None},  # missing email\n",
    "        {\"id\": 4, \"name\": \"David\", \"email\": \"david@company.org\"}  # normal\n",
    "    ]\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d60d3976-cac8-4678-82b0-190ff4666cf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform_customer_data(customer_list):\n",
    "    \"\"\"Transforms customer data, e.g., adds a domain field and makes name uppercase.\"\"\"\n",
    "    print(\"TRANSFORM: Processing customer data...\")\n",
    "    transformed_list = []\n",
    "    if not customer_list: # Basic check for empty input\n",
    "        print(\"TRANSFORM: No data to transform.\")\n",
    "        return []\n",
    "    for customer in customer_list:\n",
    "        try:\n",
    "            name_parts = customer.get(\"name\", \"\").split(\" \", 1)\n",
    "            first_name = name_parts[0] if name_parts else \"Unknown\"\n",
    "            \n",
    "            email_domain = customer.get(\"email\", \"\").split(\"@\")[-1] if \"@\" in customer.get(\"email\", \"\") else \"unknown\"\n",
    "            \n",
    "            transformed_list.append({\n",
    "                \"customer_id\": customer.get(\"id\"),\n",
    "                \"full_name_upper\": customer.get(\"name\", \"\").upper(),\n",
    "                \"first_name\": first_name,\n",
    "                \"email_domain\": email_domain\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"TRANSFORM: Error processing customer {customer.get('id')}: {e}\")\n",
    "            \n",
    "    return transformed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9f6213c-5d27-4445-ad35-b99b390e9fe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Integration Test ---\n",
    "def test_extraction_and_transformation():\n",
    "    print(\"\\n--- Running Integration Test: Extraction & Transformation ---\")\n",
    "    \n",
    "    # 1. Call the first component\n",
    "    extracted_data = extract_source_data()\n",
    "    \n",
    "    # 2. Pass its output to the second component\n",
    "    transformed_data = transform_customer_data(extracted_data)\n",
    "    \n",
    "    # 3. Validate the interaction and final output\n",
    "    print(\"\\nVALIDATE: Checking transformed data...\")\n",
    "    \n",
    "    # Check if we got any data back\n",
    "    assert len(transformed_data) == len(extracted_data), f\"Expected {len(extracted_data)} records, got {len(transformed_data)}\"\n",
    "    print(f\"Test Passed: Got {len(transformed_data)} records as expected.\")\n",
    "\n",
    "    # Check specifics of the first transformed record (example)\n",
    "    first_record = transformed_data[0]\n",
    "    expected_first_record_name = \"ALICE WONDERLAND\"\n",
    "\n",
    "    assert first_record[\"full_name_upper\"] == expected_first_record_name, \\\n",
    "        f\"Expected name '{expected_first_record_name}', got '{first_record['full_name_upper']}'\"\n",
    "    print(f\"Test Passed: First record name '{first_record['full_name_upper']}' is correct.\")\n",
    "    \n",
    "    expected_first_record_domain = \"example.com\"\n",
    "    assert first_record[\"email_domain\"] == expected_first_record_domain, \\\n",
    "        f\"Expected domain '{expected_first_record_domain}', got '{first_record['email_domain']}'\"\n",
    "    print(f\"Test Passed: First record email domain '{first_record['email_domain']}' is correct.\")\n",
    "    \n",
    "    print(\"--- Integration Test Completed Successfully! ---\")\n",
    "    # For display in notebook:\n",
    "    print(\"\\nFinal Transformed Data:\")\n",
    "    for record in transformed_data:\n",
    "        print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "363d28ca-016a-4c32-938e-9c468fa04f02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- Running Integration Test: Extraction & Transformation ---\nEXTRACT: Getting raw customer data...\nTRANSFORM: Processing customer data...\nTRANSFORM: Error processing customer 3: argument of type 'NoneType' is not iterable\n\nVALIDATE: Checking transformed data...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8085892093823536>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m test_extraction_and_transformation()\n",
       "\n",
       "File \u001B[0;32m<command-8085892093823535>, line 15\u001B[0m, in \u001B[0;36mtest_extraction_and_transformation\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mVALIDATE: Checking transformed data...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Check if we got any data back\u001B[39;00m\n",
       "\u001B[0;32m---> 15\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(transformed_data) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(extracted_data), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(extracted_data)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m records, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(transformed_data)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest Passed: Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(transformed_data)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m records as expected.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Check specifics of the first transformed record (example)\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAssertionError\u001B[0m: Expected 4 records, got 3"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AssertionError",
        "evalue": "Expected 4 records, got 3"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>AssertionError</span>: Expected 4 records, got 3"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
        "File \u001B[0;32m<command-8085892093823536>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m test_extraction_and_transformation()\n",
        "File \u001B[0;32m<command-8085892093823535>, line 15\u001B[0m, in \u001B[0;36mtest_extraction_and_transformation\u001B[0;34m()\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mVALIDATE: Checking transformed data...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Check if we got any data back\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(transformed_data) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(extracted_data), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(extracted_data)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m records, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(transformed_data)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest Passed: Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(transformed_data)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m records as expected.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Check specifics of the first transformed record (example)\u001B[39;00m\n",
        "\u001B[0;31mAssertionError\u001B[0m: Expected 4 records, got 3"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_extraction_and_transformation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9925163a-625e-4636-9014-2f6cfede813c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**The above test case indicates that due to problem in bad data, that was not identified in extract or transformed data, the record of that bad data was skipped in transformation, and that problem identified in the Integration Testing, that we expect 4 records but got only 3 records means some mistake occured in transformation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "690a6b0e-9df1-4202-aa60-16219b781fcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Boundary Testing\n",
    "\n",
    "#### WHAT IS BOUNDARY TESTING?\n",
    "\n",
    "**SIMPLE ANALOGY:** Restaurant Kitchen\n",
    "- INTERNAL BOUNDARY: Kitchen staff passing food between cooking stations\n",
    "  (Small change = Minor impact: Chef changes garnish style)\n",
    "  \n",
    "- EXTERNAL BOUNDARY: Kitchen receiving ingredients from suppliers\n",
    "  (Small change = MAJOR impact: Supplier sends wrong ingredients)\n",
    "\n",
    "IN DATA PIPELINES:\n",
    "- INTERNAL: Between our pipeline steps (we control both sides)\n",
    "- EXTERNAL: Between our pipeline and external systems (we control only one side)\n",
    "\n",
    "KEY INSIGHT: External boundary changes cause bigger problems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb14e85-a89c-4a8f-a22e-b2a2b08adfa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nPART 1: BOUNDARY TESTING\n==================================================\n\n\n\n\nTESTING INTERNAL BOUNDARY CHANGES\n----------------------------------------\nOriginal Internal Pipeline:\nCategorizing orders based on amount\nApplying discount rates based on order category\n+--------+-----------+-------+------+---------+--------------+-------------+------------+\n|order_id|customer_id|product|amount|   status|order_category|discount_rate|final_amount|\n+--------+-----------+-------+------+---------+--------------+-------------+------------+\n|  ORD001|    CUST001| Laptop|1200.0|completed|       Premium|          0.1|      1080.0|\n|  ORD002|    CUST002|  Phone| 800.0|completed|      Standard|         0.05|       760.0|\n|  ORD003|    CUST003| Tablet| 400.0|  pending|         Basic|         0.02|       392.0|\n+--------+-----------+-------+------+---------+--------------+-------------+------------+\n\n\nAfter Internal Change (Premium threshold 800 → 1000):\nApplying discount rates based on order category\n+--------+-----------+-------+------+---------+--------------+-------------+------------+\n|order_id|customer_id|product|amount|   status|order_category|discount_rate|final_amount|\n+--------+-----------+-------+------+---------+--------------+-------------+------------+\n|  ORD001|    CUST001| Laptop|1200.0|completed|       Premium|          0.1|      1080.0|\n|  ORD002|    CUST002|  Phone| 800.0|completed|       Premium|          0.1|       720.0|\n|  ORD003|    CUST003| Tablet| 400.0|  pending|         Basic|         0.02|       392.0|\n+--------+-----------+-------+------+---------+--------------+-------------+------------+\n\nRESULT: Internal change has predictable, manageable impact\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql import Row\n",
    "\n",
    "print(\"\\nPART 1: BOUNDARY TESTING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\"\"\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "def create_sample_orders():\n",
    "    \"\"\"\n",
    "    Create sample order data (simulating internal data)\n",
    "    \"\"\"\n",
    "    orders = [\n",
    "        Row(order_id=\"ORD001\", customer_id=\"CUST001\", product=\"Laptop\", amount=1200.0, status=\"completed\"),\n",
    "        Row(order_id=\"ORD002\", customer_id=\"CUST002\", product=\"Phone\", amount=800.0, status=\"completed\"),\n",
    "        Row(order_id=\"ORD003\", customer_id=\"CUST003\", product=\"Tablet\", amount=400.0, status=\"pending\")\n",
    "    ]\n",
    "    return spark.createDataFrame(orders)\n",
    "\n",
    "# internal boundary step 1\n",
    "def categorize_orders_by_amount(orders_df):\n",
    "    \"\"\"\n",
    "    Categorizes each order into 'Premium', 'Standard', or 'Basic' based on the order amount.\n",
    "\n",
    "    - 'Premium' for orders with amount >= 1000\n",
    "    - 'Standard' for orders with amount >= 500 and < 1000\n",
    "    - 'Basic' for orders with amount < 500\n",
    "    \"\"\"\n",
    "    print(\"Categorizing orders based on amount\")\n",
    "    \n",
    "    categorized_df = orders_df.withColumn(\n",
    "        \"order_category\",\n",
    "        when(col(\"amount\") >= 1000, \"Premium\")\n",
    "        .when(col(\"amount\") >= 500, \"Standard\")\n",
    "        .otherwise(\"Basic\")\n",
    "    )\n",
    "    \n",
    "    return categorized_df\n",
    "\n",
    "\n",
    "def apply_discounts_by_category(categorized_df):\n",
    "    \"\"\"\n",
    "    Applies discount rates based on the order category and calculates the final amount:\n",
    "\n",
    "    - 'Premium' orders receive a 10% discount\n",
    "    - 'Standard' orders receive a 5% discount\n",
    "    - 'Basic' orders receive a 2% discount\n",
    "\n",
    "    Adds two new columns:\n",
    "    - 'discount_rate': The applied discount rate\n",
    "    - 'final_amount': The amount after applying the discount\n",
    "    \"\"\"\n",
    "    print(\"Applying discount rates based on order category\")\n",
    "    \n",
    "    final_df = categorized_df.withColumn(\n",
    "        \"discount_rate\",\n",
    "        when(col(\"order_category\") == \"Premium\", 0.10)\n",
    "        .when(col(\"order_category\") == \"Standard\", 0.05)\n",
    "        .otherwise(0.02)\n",
    "    ).withColumn(\n",
    "        \"final_amount\",\n",
    "        col(\"amount\") * (1 - col(\"discount_rate\"))\n",
    "    )\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def test_internal_boundary_change():\n",
    "    \"\"\"\n",
    "    Test what happens when we change internal boundary (low impact)\n",
    "    \"\"\"\n",
    "    print(\"\\nTESTING INTERNAL BOUNDARY CHANGES\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Original pipeline\n",
    "    print(\"Original Internal Pipeline:\")\n",
    "    orders = create_sample_orders()\n",
    "    step1_result = categorize_orders_by_amount(orders)\n",
    "    step2_result = apply_discounts_by_category(step1_result)\n",
    "    step2_result.show()\n",
    "    \n",
    "    # Small change in internal boundary (change category threshold)\n",
    "    print(\"\\nAfter Internal Change (Premium threshold 800 → 1000):\")\n",
    "    \n",
    "    def modified_internal_step1(orders_df):\n",
    "        # Changed: Premium threshold from 1000 to 800\n",
    "        return orders_df.withColumn(\n",
    "            \"order_category\",\n",
    "            when(col(\"amount\") >= 800, \"Premium\")  # Changed threshold\n",
    "            .when(col(\"amount\") >= 500, \"Standard\")\n",
    "            .otherwise(\"Basic\")\n",
    "        )\n",
    "    \n",
    "    modified_step1 = modified_internal_step1(orders)\n",
    "    modified_step2 = apply_discounts_by_category(modified_step1)\n",
    "    modified_step2.show()\n",
    "    \n",
    "    print(\"RESULT: Internal change has predictable, manageable impact\")\n",
    "\n",
    "# Run internal boundary test\n",
    "orders_data = create_sample_orders()\n",
    "test_internal_boundary_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ce7d030-3d66-42a2-b991-9106c9cfc156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n🧪 TESTING EXTERNAL SYSTEM SCHEMA CHANGE\n----------------------------------------\n✅ Testing with External Customer Data V1:\n+--------+-----------+-------+-------------+---------------+\n|order_id|customer_id|product|customer_name| customer_email|\n+--------+-----------+-------+-------------+---------------+\n|  ORD001|    CUST001| Laptop|   John Smith| john@email.com|\n|  ORD002|    CUST002|  Phone|Sarah Johnson|sarah@email.com|\n|  ORD003|    CUST003| Tablet|   Mike Brown| mike@email.com|\n+--------+-----------+-------+-------------+---------------+\n\n\n❌ Testing with External Customer Data V2 (Changed Format):\n+--------+-----------+-------+-------------+---------------+------------+\n|order_id|customer_id|product|customer_name| customer_email|       phone|\n+--------+-----------+-------+-------------+---------------+------------+\n|  ORD001|    CUST001| Laptop|   John Smith| john@email.com|123-456-7890|\n|  ORD002|    CUST002|  Phone|Sarah Johnson|sarah@email.com|234-567-8901|\n|  ORD003|    CUST003| Tablet|   Mike Brown| mike@email.com|345-678-9012|\n+--------+-----------+-------+-------------+---------------+------------+\n\n\n🚨 RESULT: External schema changes require pipeline adjustments.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "def load_external_customer_data_v1():\n",
    "    \"\"\"\n",
    "    Loads customer data from the external system - Version 1.\n",
    "    Schema:\n",
    "    - cust_id\n",
    "    - name\n",
    "    - email\n",
    "    \"\"\"\n",
    "    customers = [\n",
    "        Row(cust_id=\"CUST001\", name=\"John Smith\", email=\"john@email.com\"),\n",
    "        Row(cust_id=\"CUST002\", name=\"Sarah Johnson\", email=\"sarah@email.com\"),\n",
    "        Row(cust_id=\"CUST003\", name=\"Mike Brown\", email=\"mike@email.com\")\n",
    "    ]\n",
    "    return spark.createDataFrame(customers)\n",
    "\n",
    "def load_external_customer_data_v2():\n",
    "    \"\"\"\n",
    "    Loads customer data from the updated external system - Version 2.\n",
    "    Schema changed:\n",
    "    - customer_id (was cust_id)\n",
    "    - full_name (was name)\n",
    "    - contact_email (was email)\n",
    "    - phone (new field)\n",
    "    \"\"\"\n",
    "    customers = [\n",
    "        Row(customer_id=\"CUST001\", full_name=\"John Smith\", contact_email=\"john@email.com\", phone=\"123-456-7890\"),\n",
    "        Row(customer_id=\"CUST002\", full_name=\"Sarah Johnson\", contact_email=\"sarah@email.com\", phone=\"234-567-8901\"),\n",
    "        Row(customer_id=\"CUST003\", full_name=\"Mike Brown\", contact_email=\"mike@email.com\", phone=\"345-678-9012\")\n",
    "    ]\n",
    "    return spark.createDataFrame(customers)\n",
    "\n",
    "def join_orders_with_customers(orders_df, customers_df, version=\"v1\"):\n",
    "    \"\"\"\n",
    "    Joins orders with customer data depending on the external system version.\n",
    "    \n",
    "    Parameters:\n",
    "    - orders_df: DataFrame containing orders\n",
    "    - customers_df: DataFrame containing customer data (schema depends on version)\n",
    "    - version: 'v1' for legacy format, 'v2' for updated format\n",
    "\n",
    "    Returns:\n",
    "    - Tuple: (joined DataFrame, join success flag)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if version == \"v1\":\n",
    "            joined_df = orders_df.join(customers_df, orders_df.customer_id == customers_df.cust_id, \"left\") \\\n",
    "                .select(\n",
    "                    orders_df[\"*\"],\n",
    "                    customers_df[\"name\"].alias(\"customer_name\"),\n",
    "                    customers_df[\"email\"].alias(\"customer_email\")\n",
    "                )\n",
    "        elif version == \"v2\":\n",
    "            joined_df = orders_df.join(customers_df, orders_df.customer_id == customers_df.customer_id, \"left\") \\\n",
    "                .select(\n",
    "                    orders_df[\"*\"],\n",
    "                    customers_df[\"full_name\"].alias(\"customer_name\"),\n",
    "                    customers_df[\"contact_email\"].alias(\"customer_email\"),\n",
    "                    customers_df[\"phone\"]\n",
    "                )\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported version passed to join logic.\")\n",
    "        \n",
    "        return joined_df, True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Join failed for version '{version}': {e}\")\n",
    "        return orders_df, False\n",
    "\n",
    "def test_external_customer_schema_change():\n",
    "    \"\"\"\n",
    "    Simulates and tests the impact of external customer schema change on the pipeline.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n🧪 TESTING EXTERNAL SYSTEM SCHEMA CHANGE\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    orders = create_sample_orders()  # Assume this is defined elsewhere\n",
    "\n",
    "    # ✅ Test with external system v1\n",
    "    print(\"✅ Testing with External Customer Data V1:\")\n",
    "    customers_v1 = load_external_customer_data_v1()\n",
    "    joined_v1, success_v1 = join_orders_with_customers(orders, customers_v1, version=\"v1\")\n",
    "    if success_v1:\n",
    "        joined_v1.select(\"order_id\", \"customer_id\", \"product\", \"customer_name\", \"customer_email\").show()\n",
    "\n",
    "    # ❌ Test with external system v2\n",
    "    print(\"\\n❌ Testing with External Customer Data V2 (Changed Format):\")\n",
    "    customers_v2 = load_external_customer_data_v2()\n",
    "    joined_v2, success_v2 = join_orders_with_customers(orders, customers_v2, version=\"v2\")\n",
    "    \n",
    "    if not success_v2:\n",
    "        print(\"💥 PIPELINE BROKEN! Could not join with new customer schema.\")\n",
    "    else:\n",
    "        joined_v2.select(\"order_id\", \"customer_id\", \"product\", \"customer_name\", \"customer_email\", \"phone\").show()\n",
    "\n",
    "    print(\"\\n🚨 RESULT: External schema changes require pipeline adjustments.\")\n",
    "\n",
    "# Run the test\n",
    "test_external_customer_schema_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d6d9136-39e8-42eb-afaa-dd968c7aa752",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n🧪 TESTING EXTERNAL SYSTEM SCHEMA CHANGE\n----------------------------------------\n✅ Testing with External Customer Data V1:\n+--------+-----------+-------+-------------+---------------+\n|order_id|customer_id|product|customer_name| customer_email|\n+--------+-----------+-------+-------------+---------------+\n|  ORD001|    CUST001| Laptop|   John Smith| john@email.com|\n|  ORD002|    CUST002|  Phone|Sarah Johnson|sarah@email.com|\n|  ORD003|    CUST003| Tablet|   Mike Brown| mike@email.com|\n+--------+-----------+-------+-------------+---------------+\n\n\n❌ Testing with External Customer Data V2 (Changed Format):\n❌ Join failed: [ATTRIBUTE_NOT_SUPPORTED] Attribute `cust_id` is not supported.\n💥 PIPELINE BROKEN! Could not join with new customer schema.\n\n🚨 RESULT: External schema changes require pipeline adjustments.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "def load_external_customer_data_v1():\n",
    "    \"\"\"\n",
    "    Loads customer data from the external system - Version 1.\n",
    "    Schema:\n",
    "    - cust_id\n",
    "    - name\n",
    "    - email\n",
    "    \"\"\"\n",
    "    customers = [\n",
    "        Row(cust_id=\"CUST001\", name=\"John Smith\", email=\"john@email.com\"),\n",
    "        Row(cust_id=\"CUST002\", name=\"Sarah Johnson\", email=\"sarah@email.com\"),\n",
    "        Row(cust_id=\"CUST003\", name=\"Mike Brown\", email=\"mike@email.com\")\n",
    "    ]\n",
    "    return spark.createDataFrame(customers)\n",
    "\n",
    "def load_external_customer_data_v2():\n",
    "    \"\"\"\n",
    "    Loads customer data from the updated external system - Version 2.\n",
    "    Schema changed:\n",
    "    - customer_id (was cust_id)\n",
    "    - full_name (was name)\n",
    "    - contact_email (was email)\n",
    "    - phone (new field)\n",
    "    \"\"\"\n",
    "    customers = [\n",
    "        Row(id=\"CUST001\", full_name=\"John Smith\", contact_email=\"john@email.com\", phone=\"123-456-7890\"),\n",
    "        Row(id=\"CUST002\", full_name=\"Sarah Johnson\", contact_email=\"sarah@email.com\", phone=\"234-567-8901\"),\n",
    "        Row(id=\"CUST003\", full_name=\"Mike Brown\", contact_email=\"mike@email.com\", phone=\"345-678-9012\")\n",
    "    ]\n",
    "    return spark.createDataFrame(customers)\n",
    "\n",
    "def join_orders_with_customers(orders_df, customers_df):\n",
    "    \"\"\"\n",
    "    Joins orders with customer data depending on the external system version.\n",
    "    \n",
    "    Parameters:\n",
    "    - orders_df: DataFrame containing orders\n",
    "    - customers_df: DataFrame containing customer data (schema depends on version)\n",
    "\n",
    "    Returns:\n",
    "    - Tuple: (joined DataFrame, join success flag)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        joined_df = orders_df.join(customers_df, orders_df.customer_id == customers_df.cust_id, \"left\") \\\n",
    "                .select(\n",
    "                    orders_df[\"*\"],\n",
    "                    customers_df[\"name\"].alias(\"customer_name\"),\n",
    "                    customers_df[\"email\"].alias(\"customer_email\")\n",
    "        )\n",
    "        \n",
    "        return joined_df, True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Join failed: {e}\")\n",
    "        return orders_df, False\n",
    "\n",
    "def test_external_customer_schema_change():\n",
    "    \"\"\"\n",
    "    Simulates and tests the impact of external customer schema change on the pipeline.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n🧪 TESTING EXTERNAL SYSTEM SCHEMA CHANGE\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    orders = create_sample_orders()  # Assume this is defined elsewhere\n",
    "\n",
    "    # ✅ Test with external system v1\n",
    "    print(\"✅ Testing with External Customer Data V1:\")\n",
    "    customers_v1 = load_external_customer_data_v1()\n",
    "    joined_v1, success_v1 = join_orders_with_customers(orders, customers_v1)\n",
    "    if success_v1:\n",
    "        joined_v1.select(\"order_id\", \"customer_id\", \"product\", \"customer_name\", \"customer_email\").show()\n",
    "\n",
    "    # ❌ Test with external system v2\n",
    "    print(\"\\n❌ Testing with External Customer Data V2 (Changed Format):\")\n",
    "    customers_v2 = load_external_customer_data_v2()\n",
    "    joined_v2, success_v2 = join_orders_with_customers(orders, customers_v2)\n",
    "    \n",
    "    if not success_v2:\n",
    "        print(\"💥 PIPELINE BROKEN! Could not join with new customer schema.\")\n",
    "    else:\n",
    "        joined_v2.select(\"order_id\", \"customer_id\", \"product\", \"customer_name\", \"customer_email\", \"phone\").show()\n",
    "\n",
    "    print(\"\\n🚨 RESULT: External schema changes require pipeline adjustments.\")\n",
    "\n",
    "# Run the test\n",
    "test_external_customer_schema_change()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c11f851-e0f3-483d-887e-77012e3ad1dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Dependency Testing\n",
    "\n",
    "WHAT IS DEPENDENCY TESTING?\n",
    "\n",
    "SIMPLE ANALOGY: Assembly Line Factory\n",
    "- Raw materials arrive in specific format (schema)\n",
    "- If supplier changes material format → assembly line breaks\n",
    "- If materials arrive late → production delays\n",
    "\n",
    "IN DATA PIPELINES:\n",
    "- Upstream system changes schema → our transformations break\n",
    "- Data arrives late → downstream processes affected\n",
    "- New columns added → logic might ignore them\n",
    "\n",
    "KEY TYPES:\n",
    "1. Schema Changes (column names, data types)\n",
    "2. Data Freshness (timing of data arrival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59a47a23-a44a-4827-b2bb-6733d68421d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTESTING SCHEMA DEPENDENCY\n----------------------------------------\nTesting with Old Schema:\nOld Schema Columns: ['txn_id', 'customer', 'amount', 'date']\n+------+--------+------+----------+\n|txn_id|customer|amount|      date|\n+------+--------+------+----------+\n|TXN001| CUST001| 100.5|2024-01-15|\n|TXN002| CUST002|250.75|2024-01-16|\n|TXN003| CUST003| 75.25|2024-01-17|\n+------+--------+------+----------+\n\nTransformation Result:\n+--------+------------+-----------------+\n|customer|total_amount|transaction_count|\n+--------+------------+-----------------+\n| CUST001|       100.5|                1|\n| CUST002|      250.75|                1|\n| CUST003|       75.25|                1|\n+--------+------------+-----------------+\n\n\nTesting New Schema with Old Logic:\nNew Schema Columns: ['transaction_id', 'customer_id', 'transaction_amount', 'transaction_date', 'transaction_type', 'currency']\n+--------------+-----------+------------------+----------------+----------------+--------+\n|transaction_id|customer_id|transaction_amount|transaction_date|transaction_type|currency|\n+--------------+-----------+------------------+----------------+----------------+--------+\n|        TXN001|    CUST001|             100.5|      2024-01-15|        purchase|     USD|\n|        TXN002|    CUST002|            250.75|      2024-01-16|        purchase|     USD|\n|        TXN003|    CUST003|             75.25|      2024-01-17|          refund|     USD|\n+--------------+-----------+------------------+----------------+----------------+--------+\n\n❌ Transformation failed: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `customer` cannot be resolved. Did you mean one of the following? [`customer_id`, `currency`, `transaction_id`, `transaction_date`, `transaction_type`]. SQLSTATE: 42703;\n'Aggregate ['customer], ['customer, 'sum('amount) AS total_amount#583, 'count('txn_id) AS transaction_count#584]\n+- LogicalRDD [transaction_id#540, customer_id#541, transaction_amount#542, transaction_date#543, transaction_type#544, currency#545], false\n\n\nTesting New Schema with Flexible Logic:\n📋 Detected new schema format\nFlexible Transformation Result:\n+-----------+------------+-----------------+\n|customer_id|total_amount|transaction_count|\n+-----------+------------+-----------------+\n|    CUST001|       100.5|                1|\n|    CUST002|      250.75|                1|\n|    CUST003|       75.25|                1|\n+-----------+------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count\n",
    "\n",
    "def create_upstream_data_old_schema():\n",
    "    \"\"\"\n",
    "    Upstream system original schema\n",
    "    \"\"\"\n",
    "    transactions = [\n",
    "        Row(txn_id=\"TXN001\", customer=\"CUST001\", amount=100.50, date=\"2024-01-15\"),\n",
    "        Row(txn_id=\"TXN002\", customer=\"CUST002\", amount=250.75, date=\"2024-01-16\"),\n",
    "        Row(txn_id=\"TXN003\", customer=\"CUST003\", amount=75.25, date=\"2024-01-17\")\n",
    "    ]\n",
    "    return spark.createDataFrame(transactions)\n",
    "\n",
    "def create_upstream_data_new_schema():\n",
    "    \"\"\"\n",
    "    Upstream system changed schema! Added new columns, changed types\n",
    "    \"\"\"\n",
    "    transactions = [\n",
    "        Row(transaction_id=\"TXN001\", customer_id=\"CUST001\", transaction_amount=100.50, \n",
    "            transaction_date=\"2024-01-15\", transaction_type=\"purchase\", currency=\"USD\"),\n",
    "        Row(transaction_id=\"TXN002\", customer_id=\"CUST002\", transaction_amount=250.75, \n",
    "            transaction_date=\"2024-01-16\", transaction_type=\"purchase\", currency=\"USD\"),\n",
    "        Row(transaction_id=\"TXN003\", customer_id=\"CUST003\", transaction_amount=75.25, \n",
    "            transaction_date=\"2024-01-17\", transaction_type=\"refund\", currency=\"USD\")\n",
    "    ]\n",
    "    return spark.createDataFrame(transactions)\n",
    "\n",
    "def our_transformation_logic_v1(transactions_df):\n",
    "    \"\"\"\n",
    "    Our original transformation logic (expects old schema)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Our logic expects: txn_id, customer, amount, date\n",
    "        summary = transactions_df.groupBy(\"customer\").agg(\n",
    "            sum(\"amount\").alias(\"total_amount\"),\n",
    "            count(\"txn_id\").alias(\"transaction_count\")\n",
    "        )\n",
    "        return summary, True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Transformation failed: {e}\")\n",
    "        return None, False\n",
    "\n",
    "def our_transformation_logic_v2(transactions_df):\n",
    "    \"\"\"\n",
    "    Updated transformation logic (handles schema changes gracefully)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check which schema we're dealing with\n",
    "        columns = transactions_df.columns\n",
    "        \n",
    "        if \"txn_id\" in columns:\n",
    "            # Old schema\n",
    "            print(\"📋 Detected old schema format\")\n",
    "            customer_col = \"customer\"\n",
    "            amount_col = \"amount\"\n",
    "            id_col = \"txn_id\"\n",
    "        elif \"transaction_id\" in columns:\n",
    "            # New schema\n",
    "            print(\"📋 Detected new schema format\")\n",
    "            customer_col = \"customer_id\"\n",
    "            amount_col = \"transaction_amount\"\n",
    "            id_col = \"transaction_id\"\n",
    "        else:\n",
    "            raise Exception(\"Unknown schema format!\")\n",
    "        \n",
    "        # Flexible transformation\n",
    "        summary = transactions_df.groupBy(customer_col).agg(\n",
    "            sum(amount_col).alias(\"total_amount\"),\n",
    "            count(id_col).alias(\"transaction_count\")\n",
    "        )\n",
    "        \n",
    "        return summary, True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Flexible transformation failed: {e}\")\n",
    "        return None, False\n",
    "\n",
    "def test_schema_dependency():\n",
    "    \"\"\"\n",
    "    Test how schema changes affect our pipeline\n",
    "    \"\"\"\n",
    "    print(\"\\nTESTING SCHEMA DEPENDENCY\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Test with old schema\n",
    "    print(\"Testing with Old Schema:\")\n",
    "    old_data = create_upstream_data_old_schema()\n",
    "    print(\"Old Schema Columns:\", old_data.columns)\n",
    "    old_data.show(3)\n",
    "    \n",
    "    result_old, success_old = our_transformation_logic_v1(old_data)\n",
    "    if success_old:\n",
    "        print(\"Transformation Result:\")\n",
    "        result_old.show()\n",
    "    \n",
    "    # Test with new schema using old logic\n",
    "    print(\"\\nTesting New Schema with Old Logic:\")\n",
    "    new_data = create_upstream_data_new_schema()\n",
    "    print(\"New Schema Columns:\", new_data.columns)\n",
    "    new_data.show(3)\n",
    "    \n",
    "    result_new, success_new = our_transformation_logic_v1(new_data)\n",
    "    \n",
    "    # Test with new schema using flexible logic\n",
    "    print(\"\\nTesting New Schema with Flexible Logic:\")\n",
    "    result_flexible, success_flexible = our_transformation_logic_v2(new_data)\n",
    "    if success_flexible:\n",
    "        print(\"Flexible Transformation Result:\")\n",
    "        result_flexible.show()\n",
    "\n",
    "test_schema_dependency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2609951e-022c-480e-b4c6-799509dcf44d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "So we do schema testing in boundary testing also, but in that our main aim is to focus on the data accuracy, completeness, data types, and the data other component accepts is properly passed by the previous component. These all testing takes place in Boundary testing. So normally data types checks, schema checks etc. we do in internal boundary testing and external boundary testing both. But the **goal in boundary testing** is: To ensure the **handoff** between components is reliable — meaning the _correct data_ is passed _in the expected structure_, with proper **accuracy**, **completeness**, and **types**.\n",
    "\n",
    "\n",
    "But schema checking plays a critical role in dependency testing, as the main aim of dependency testing is to test that any change in external source doesn't crash the pipeline. So if there are any unexpected changes like change in structure, change in API urls, network connections, credentials changes, etc. So any changes on which the system depends shouldn't crash the entire pipeline, instead it should handle the cases with proper logs\n",
    "\n",
    "In **dependency testing**, schema checking is not just about _validation_ — it’s about **guarding against breakage** due to changes in **external systems** your pipeline depends on.\n",
    "\n",
    "The aim is:\n",
    "\n",
    "> ⚠️ _\"If the schema of an external source/API changes, your pipeline should either adapt or fail gracefully — not crash silently or corrupt data.\"_\n",
    "\n",
    "That’s why **dependency testing** also involves:\n",
    "\n",
    "- **Error handling**\n",
    "- **Logging**\n",
    "- **Graceful fallback or fail-fast mechanisms**\n",
    "- **Alerts**\n",
    "    \n",
    "So you nailed it — schema testing becomes **critical** in dependency testing because **external systems are brittle** and **outside your control**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47d96894-304a-43a8-96b6-d93d5d3c4b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Idempotence Testing\n",
    "\n",
    "WHAT IS END-TO-END TESTING?\n",
    "\n",
    "SIMPLE ANALOGY: Bank Account Balance\n",
    "- You deposit $100 → Balance should increase by $100\n",
    "- You run same deposit again → Balance should NOT change again (idempotent)\n",
    "- Only new transactions should change balance\n",
    "\n",
    "IN DATA PIPELINES:\n",
    "- Same source data → Same target result (idempotent)\n",
    "- Only approved source changes → Target changes\n",
    "- Complete pipeline validation (schema + data + business rules)\n",
    "\n",
    "KEY CONCEPTS:\n",
    "1. Idempotence (repeatable results)\n",
    "2. Change approval (controlled updates)\n",
    "3. Complete data journey validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a454d07-a40b-45f5-9d19-ad64eb814063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n🔄 TESTING IDEMPOTENCE\n----------------------------------------\n📊 Source Data:\n+-----------+------+\n|customer_id|orders|\n+-----------+------+\n|          1|     2|\n|          3|     4|\n+-----------+------+\n\n📊 Initial Target Data:\n+-----------+------------+\n|customer_id|total_orders|\n+-----------+------------+\n|          1|           5|\n|          2|          10|\n|          3|          15|\n+-----------+------------+\n\n🔄 First Pipeline Run:\n+-----------+------------+\n|customer_id|total_orders|\n+-----------+------------+\n|          1|           7|\n|          2|          10|\n|          3|          19|\n+-----------+------------+\n\n🔄 Second Pipeline Run (Same Source):\n+-----------+------------+\n|customer_id|total_orders|\n+-----------+------------+\n|          1|           9|\n|          2|          10|\n|          3|          23|\n+-----------+------------+\n\n🎯 Idempotence Test: ❌ FAILED\n💥 PROBLEM: Pipeline is not idempotent! Results change on re-run\n"
     ]
    }
   ],
   "source": [
    "def create_source_orders():\n",
    "    \"\"\"\n",
    "    Source order data that should produce consistent results\n",
    "    \"\"\"\n",
    "    orders = [\n",
    "        Row(customer_id=1, orders=2),\n",
    "        Row(customer_id=3, orders=4)\n",
    "    ]\n",
    "    return spark.createDataFrame(orders)\n",
    "\n",
    "def create_existing_target():\n",
    "    \"\"\"\n",
    "    Existing aggregated target data\n",
    "    \"\"\"\n",
    "    target = [\n",
    "        Row(customer_id=1, total_orders=5),\n",
    "        Row(customer_id=2, total_orders=10),\n",
    "        Row(customer_id=3, total_orders=15)\n",
    "    ]\n",
    "    return spark.createDataFrame(target)\n",
    "\n",
    "def upsert_logic(source_df, target_df):\n",
    "    \"\"\"\n",
    "    Upsert logic: Update existing customers, insert new ones\n",
    "    \"\"\"\n",
    "    # This logic should be idempotent\n",
    "    \n",
    "    # Join source with target to find updates\n",
    "    joined = source_df.alias(\"src\").join(\n",
    "        target_df.alias(\"tgt\"), \n",
    "        col(\"src.customer_id\") == col(\"tgt.customer_id\"), \n",
    "        \"full_outer\"\n",
    "    )\n",
    "    \n",
    "    # Calculate new totals\n",
    "    result = joined.select(\n",
    "        when(col(\"src.customer_id\").isNotNull(), col(\"src.customer_id\"))\n",
    "        .otherwise(col(\"tgt.customer_id\")).alias(\"customer_id\"),\n",
    "        \n",
    "        when(col(\"src.customer_id\").isNotNull(), \n",
    "             col(\"tgt.total_orders\") + col(\"src.orders\"))\n",
    "        .otherwise(col(\"tgt.total_orders\")).alias(\"total_orders\")\n",
    "    ).filter(col(\"customer_id\").isNotNull())\n",
    "    \n",
    "    return result\n",
    "\n",
    "def test_idempotence():\n",
    "    \"\"\"\n",
    "    Test that running pipeline multiple times gives same result\n",
    "    \"\"\"\n",
    "    print(\"\\n🔄 TESTING IDEMPOTENCE\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    source = create_source_orders()\n",
    "    initial_target = create_existing_target()\n",
    "    \n",
    "    print(\"📊 Source Data:\")\n",
    "    source.show()\n",
    "    \n",
    "    print(\"📊 Initial Target Data:\")\n",
    "    initial_target.show()\n",
    "    \n",
    "    # First run\n",
    "    print(\"🔄 First Pipeline Run:\")\n",
    "    result1 = upsert_logic(source, initial_target)\n",
    "    result1 = result1.orderBy(\"customer_id\")\n",
    "    result1.show()\n",
    "    \n",
    "    # Second run with same source (should be idempotent)\n",
    "    print(\"🔄 Second Pipeline Run (Same Source):\")\n",
    "    result2 = upsert_logic(source, result1)\n",
    "    result2 = result2.orderBy(\"customer_id\")\n",
    "    result2.show()\n",
    "    \n",
    "    # Check if results are identical\n",
    "    result1_data = [row.asDict() for row in result1.collect()]\n",
    "    result2_data = [row.asDict() for row in result2.collect()]\n",
    "    \n",
    "    is_idempotent = result1_data == result2_data\n",
    "    print(f\"🎯 Idempotence Test: {'✅ PASSED' if is_idempotent else '❌ FAILED'}\")\n",
    "    \n",
    "    if not is_idempotent:\n",
    "        print(\"💥 PROBLEM: Pipeline is not idempotent! Results change on re-run\")\n",
    "    else:\n",
    "        print(\"✅ GOOD: Pipeline is idempotent - same input gives same output\")\n",
    "\n",
    "test_idempotence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab0e0a80-fee6-4e6d-9bcc-4280f156da62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MockDatabase:\n",
    "    \"\"\"Simple mock database for testing ELT operations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # In-memory storage to simulate database tables\n",
    "        self.tables = {\n",
    "            'customers': [\n",
    "                {'id': 1, 'name': 'John Doe', 'email': 'john@email.com'},\n",
    "                {'id': 2, 'name': 'Jane Smith', 'email': 'jane@email.com'}\n",
    "            ],\n",
    "            'orders': [\n",
    "                {'order_id': 101, 'customer_id': 1, 'amount': 250.00, 'status': 'completed'},\n",
    "                {'order_id': 102, 'customer_id': 2, 'amount': 150.50, 'status': 'pending'}\n",
    "            ]\n",
    "        }\n",
    "        self.is_connected = False\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Simulate database connection\"\"\"\n",
    "        print(\"Connecting to database...\")\n",
    "        self.is_connected = True\n",
    "        return \"Connection successful\"\n",
    "    \n",
    "    def disconnect(self):\n",
    "        \"\"\"Simulate database disconnection\"\"\"\n",
    "        print(\"Disconnecting from database...\")\n",
    "        self.is_connected = False\n",
    "    \n",
    "    def execute_query(self, query):\n",
    "        \"\"\"Simulate SQL query execution\"\"\"\n",
    "        if not self.is_connected:\n",
    "            raise Exception(\"Database not connected\")\n",
    "        \n",
    "        # Simple query simulation\n",
    "        if \"SELECT * FROM customers\" in query:\n",
    "            return self.tables['customers']\n",
    "        elif \"SELECT * FROM orders\" in query:\n",
    "            return self.tables['orders']\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    def insert_data(self, table_name, data):\n",
    "        \"\"\"Simulate data insertion\"\"\"\n",
    "        if not self.is_connected:\n",
    "            raise Exception(\"Database not connected\")\n",
    "        \n",
    "        if table_name in self.tables:\n",
    "            self.tables[table_name].append(data)\n",
    "            return f\"Data inserted into {table_name}\"\n",
    "        else:\n",
    "            raise Exception(f\"Table {table_name} not found\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Mock REST API Service\n",
    "# ============================================================================\n",
    "\n",
    "class MockAPIService:\n",
    "    \"\"\"Simple mock REST API for testing external service calls\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Mock API data\n",
    "        self.api_data = {\n",
    "            'users': [\n",
    "                {'user_id': 1, 'username': 'alice', 'active': True},\n",
    "                {'user_id': 2, 'username': 'bob', 'active': False}\n",
    "            ],\n",
    "            'products': [\n",
    "                {'product_id': 'P001', 'name': 'Laptop', 'price': 999.99},\n",
    "                {'product_id': 'P002', 'name': 'Mouse', 'price': 25.00}\n",
    "            ]\n",
    "        }\n",
    "        self.request_count = 0\n",
    "    \n",
    "    def get_data(self, endpoint):\n",
    "        \"\"\"Simulate GET request to API endpoint\"\"\"\n",
    "        self.request_count += 1\n",
    "        print(f\"API Request #{self.request_count} to endpoint: {endpoint}\")\n",
    "        \n",
    "        if endpoint == '/users':\n",
    "            return {'status': 200, 'data': self.api_data['users']}\n",
    "        elif endpoint == '/products':\n",
    "            return {'status': 200, 'data': self.api_data['products']}\n",
    "        else:\n",
    "            return {'status': 404, 'error': 'Endpoint not found'}\n",
    "    \n",
    "    def post_data(self, endpoint, data):\n",
    "        \"\"\"Simulate POST request to API\"\"\"\n",
    "        self.request_count += 1\n",
    "        print(f\"API POST Request #{self.request_count} to {endpoint}\")\n",
    "        \n",
    "        if endpoint == '/users':\n",
    "            new_id = len(self.api_data['users']) + 1\n",
    "            data['user_id'] = new_id\n",
    "            self.api_data['users'].append(data)\n",
    "            return {'status': 201, 'message': 'User created', 'data': data}\n",
    "        else:\n",
    "            return {'status': 400, 'error': 'Invalid endpoint'}\n",
    "\n",
    "# ============================================================================\n",
    "# 3. Mock File System\n",
    "# ============================================================================\n",
    "\n",
    "class MockFileSystem:\n",
    "    \"\"\"Simple mock file system for testing file operations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # In-memory file storage\n",
    "        self.files = {\n",
    "            '/data/input/customers.csv': 'id,name,email\\n1,John,john@email.com\\n2,Jane,jane@email.com',\n",
    "            '/data/input/sales.json': '{\"sales\": [{\"id\": 1, \"amount\": 100}, {\"id\": 2, \"amount\": 200}]}'\n",
    "        }\n",
    "    \n",
    "    def file_exists(self, file_path):\n",
    "        \"\"\"Check if file exists\"\"\"\n",
    "        return file_path in self.files\n",
    "    \n",
    "    def read_file(self, file_path):\n",
    "        \"\"\"Read file content\"\"\"\n",
    "        if file_path in self.files:\n",
    "            return self.files[file_path]\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    def write_file(self, file_path, content):\n",
    "        \"\"\"Write content to file\"\"\"\n",
    "        self.files[file_path] = content\n",
    "        return f\"File written: {file_path}\"\n",
    "    \n",
    "    def list_files(self, directory):\n",
    "        \"\"\"List files in directory\"\"\"\n",
    "        files_in_dir = []\n",
    "        for file_path in self.files.keys():\n",
    "            if file_path.startswith(directory):\n",
    "                files_in_dir.append(file_path)\n",
    "        return files_in_dir\n",
    "\n",
    "# ============================================================================\n",
    "# 4. Integration Test Examples\n",
    "# ============================================================================\n",
    "\n",
    "def test_database_integration():\n",
    "    \"\"\"Example integration test for database operations\"\"\"\n",
    "    print(\"=== Testing Database Integration ===\")\n",
    "    \n",
    "    # Create mock database\n",
    "    db = MockDatabase()\n",
    "    \n",
    "    # Test connection\n",
    "    db.connect()\n",
    "    \n",
    "    # Test data retrieval\n",
    "    customers = db.execute_query(\"SELECT * FROM customers\")\n",
    "    print(f\"Retrieved {len(customers)} customers\")\n",
    "    \n",
    "    # Test data insertion\n",
    "    new_customer = {'id': 3, 'name': 'Bob Johnson', 'email': 'bob@email.com'}\n",
    "    result = db.insert_data('customers', new_customer)\n",
    "    print(result)\n",
    "    \n",
    "    # Verify insertion\n",
    "    updated_customers = db.execute_query(\"SELECT * FROM customers\")\n",
    "    print(f\"Now have {len(updated_customers)} customers\")\n",
    "    \n",
    "    db.disconnect()\n",
    "    print(\"Database test completed\\n\")\n",
    "\n",
    "def test_api_integration():\n",
    "    \"\"\"Example integration test for API service\"\"\"\n",
    "    print(\"=== Testing API Integration ===\")\n",
    "    \n",
    "    # Create mock API service\n",
    "    api = MockAPIService()\n",
    "    \n",
    "    # Test GET request\n",
    "    response = api.get_data('/users')\n",
    "    print(f\"API Response: {response['status']}\")\n",
    "    print(f\"Users retrieved: {len(response['data'])}\")\n",
    "    \n",
    "    # Test POST request\n",
    "    new_user = {'username': 'charlie', 'active': True}\n",
    "    post_response = api.post_data('/users', new_user)\n",
    "    print(f\"POST Response: {post_response['status']}\")\n",
    "    \n",
    "    print(f\"Total API requests made: {api.request_count}\")\n",
    "    print(\"API test completed\\n\")\n",
    "\n",
    "def test_file_system_integration():\n",
    "    \"\"\"Example integration test for file operations\"\"\"\n",
    "    print(\"=== Testing File System Integration ===\")\n",
    "    \n",
    "    # Create mock file system\n",
    "    fs = MockFileSystem()\n",
    "    \n",
    "    # Test file existence\n",
    "    csv_file = '/data/input/customers.csv'\n",
    "    if fs.file_exists(csv_file):\n",
    "        print(f\"File exists: {csv_file}\")\n",
    "        \n",
    "        # Test file reading\n",
    "        content = fs.read_file(csv_file)\n",
    "        lines = content.split('\\n')\n",
    "        print(f\"File has {len(lines)} lines\")\n",
    "    \n",
    "    # Test file writing\n",
    "    output_file = '/data/output/processed_data.txt'\n",
    "    fs.write_file(output_file, 'Processing completed successfully')\n",
    "    \n",
    "    # Test directory listing\n",
    "    input_files = fs.list_files('/data/input/')\n",
    "    print(f\"Files in input directory: {len(input_files)}\")\n",
    "    \n",
    "    print(\"File system test completed\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. Complete ELT Pipeline Mock Test\n",
    "# ============================================================================\n",
    "\n",
    "def test_complete_elt_pipeline():\n",
    "    \"\"\"Example of testing complete ELT pipeline with mocks\"\"\"\n",
    "    print(\"=== Testing Complete ELT Pipeline ===\")\n",
    "    \n",
    "    # Initialize all mock systems\n",
    "    db = MockDatabase()\n",
    "    api = MockAPIService()\n",
    "    fs = MockFileSystem()\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Extract data from API\n",
    "        print(\"Step 1: Extracting data from API...\")\n",
    "        api_data = api.get_data('/users')\n",
    "        users = api_data['data']\n",
    "        \n",
    "        # Step 2: Load data from file system\n",
    "        print(\"Step 2: Loading data from file system...\")\n",
    "        csv_content = fs.read_file('/data/input/customers.csv')\n",
    "        \n",
    "        # Step 3: Transform data (simple example)\n",
    "        print(\"Step 3: Transforming data...\")\n",
    "        active_users = [user for user in users if user['active']]\n",
    "        print(f\"Found {len(active_users)} active users\")\n",
    "        \n",
    "        # Step 4: Load transformed data to database\n",
    "        print(\"Step 4: Loading data to database...\")\n",
    "        db.connect()\n",
    "        for user in active_users:\n",
    "            customer_data = {\n",
    "                'id': user['user_id'] + 100,  # Transform ID\n",
    "                'name': user['username'],\n",
    "                'email': f\"{user['username']}@company.com\"\n",
    "            }\n",
    "            db.insert_data('customers', customer_data)\n",
    "        \n",
    "        # Step 5: Verify results\n",
    "        print(\"Step 5: Verifying results...\")\n",
    "        all_customers = db.execute_query(\"SELECT * FROM customers\")\n",
    "        print(f\"Total customers in database: {len(all_customers)}\")\n",
    "        \n",
    "        # Step 6: Write summary report\n",
    "        summary = f\"ELT Pipeline completed successfully\\nProcessed {len(active_users)} active users\"\n",
    "        fs.write_file('/data/output/pipeline_summary.txt', summary)\n",
    "        \n",
    "        print(\"ELT Pipeline test completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline test failed: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        db.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38bdbdf1-eeab-47bf-a200-9cdd852bee80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mock External Systems for Integration Testing\n==================================================\n=== Testing Database Integration ===\nConnecting to database...\nRetrieved 2 customers\nData inserted into customers\nNow have 3 customers\nDisconnecting from database...\nDatabase test completed\n\n=== Testing API Integration ===\nAPI Request #1 to endpoint: /users\nAPI Response: 200\nUsers retrieved: 2\nAPI POST Request #2 to /users\nPOST Response: 201\nTotal API requests made: 2\nAPI test completed\n\n=== Testing File System Integration ===\nFile exists: /data/input/customers.csv\nFile has 3 lines\nFiles in input directory: 2\nFile system test completed\n\n=== Testing Complete ELT Pipeline ===\nStep 1: Extracting data from API...\nAPI Request #1 to endpoint: /users\nStep 2: Loading data from file system...\nStep 3: Transforming data...\nFound 1 active users\nStep 4: Loading data to database...\nConnecting to database...\nStep 5: Verifying results...\nTotal customers in database: 3\nELT Pipeline test completed successfully!\nDisconnecting from database...\n\nAll integration tests completed!\n\nKey Learning Points:\n1. Mock objects simulate real external systems\n2. Tests can run without actual external dependencies\n3. Easy to control test data and scenarios\n4. Fast execution and reliable results\n5. Can test error conditions safely\n"
     ]
    }
   ],
   "source": [
    "print(\"Mock External Systems for Integration Testing\")\n",
    "print(\"=\" * 50)\n",
    "    \n",
    "# Run individual tests\n",
    "test_database_integration()\n",
    "test_api_integration()\n",
    "test_file_system_integration()\n",
    "    \n",
    "# Run complete pipeline test\n",
    "test_complete_elt_pipeline()\n",
    "    \n",
    "print(\"\\nAll integration tests completed!\")\n",
    "print(\"\\nKey Learning Points:\")\n",
    "print(\"1. Mock objects simulate real external systems\")\n",
    "print(\"2. Tests can run without actual external dependencies\")\n",
    "print(\"3. Easy to control test data and scenarios\")\n",
    "print(\"4. Fast execution and reliable results\")\n",
    "print(\"5. Can test error conditions safely\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab44c75f-c87d-42ff-b6e8-7a219d3328f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b087337c-8ccd-468d-b3a7-7cd876a2f745",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faker\n  Obtaining dependency information for faker from https://files.pythonhosted.org/packages/ce/99/045b2dae19a01b9fbb23b9971bc04f4ef808e7f3a213d08c81067304a210/faker-37.3.0-py3-none-any.whl.metadata\n  Downloading faker-37.3.0-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: tzdata in /databricks/python3/lib/python3.11/site-packages (from faker) (2022.1)\nDownloading faker-37.3.0-py3-none-any.whl (1.9 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.9 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.9 MB\u001B[0m \u001B[31m1.5 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\n\u001B[2K   \u001B[91m━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.1/1.9 MB\u001B[0m \u001B[31m1.1 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\n\u001B[2K   \u001B[91m━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.1/1.9 MB\u001B[0m \u001B[31m1.4 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\n\u001B[2K   \u001B[91m━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.3/1.9 MB\u001B[0m \u001B[31m2.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.6/1.9 MB\u001B[0m \u001B[31m3.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.9/1.9 MB\u001B[0m \u001B[31m4.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━\u001B[0m \u001B[32m1.5/1.9 MB\u001B[0m \u001B[31m6.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m1.9/1.9 MB\u001B[0m \u001B[31m7.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.9/1.9 MB\u001B[0m \u001B[31m7.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: faker\nSuccessfully installed faker-37.3.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f03da46-72d3-41d1-aa19-886e9ee837a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, sum as spark_sum, count, when, lit, avg, max as spark_max\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Import Faker for realistic data generation\n",
    "from faker import Faker\n",
    "from faker.providers import internet, person, company, automotive, date_time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from unittest.mock import MagicMock\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b5e3a48-de83-4de8-9f13-008541ab6b05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Faker\n",
    "fake = Faker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df6cdc2b-63d5-43ea-8005-4e0f02c7feeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎲 Basic Faker Demonstration\n------------------------------\n🧑 Personal Information:\n  Name: Brandon Murray\n  Email: joannesmith@example.com\n  Phone: (972)230-1849x34170\n  Address: 7556 Terrell Pines Apt. 607\nNguyenhaven, WI 09297\n  Birthday: 2003-11-05\n\n  Name: Andrew Leblanc\n  Email: gonzalezanthony@example.net\n  Phone: 278-736-4747\n  Address: 330 Kristin Union\nPatrickport, PA 79662\n  Birthday: 2001-08-06\n\n  Name: Chelsea Morton\n  Email: lauraking@example.org\n  Phone: (347)797-9449x4936\n  Address: 4755 Valerie Mission\nDavidstad, IL 62529\n  Birthday: 1997-10-09\n\n🏢 Business Information:\n  Company: Gordon Inc\n  Job: Counselling psychologist\n  Department: engage world-class users\n\n  Company: Duncan-Edwards\n  Job: Engineer, automotive\n  Department: whiteboard next-generation action-items\n\n  Company: Meyers, Perez and Jackson\n  Job: Drilling engineer\n  Department: deploy magnetic experiences\n\n💰 Financial Information:\n  Credit Card: 2223701888523248\n  Bank Account: GB70SIUA24617499227231\n  Currency: ETB\n\n  Credit Card: 372346627686279\n  Bank Account: GB06DERU40484262612143\n  Currency: RUB\n\n  Credit Card: 3552403062531365\n  Bank Account: GB12SFYO02692512848006\n  Currency: JPY\n\n✅ Faker generates realistic, random data every time!\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_faker_basics():\n",
    "    \"\"\"\n",
    "    Show basic Faker capabilities for generating realistic data\n",
    "    \"\"\"\n",
    "    print(\"🎲 Basic Faker Demonstration\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    print(\"🧑 Personal Information:\")\n",
    "    for i in range(3):\n",
    "        print(f\"  Name: {fake.name()}\")\n",
    "        print(f\"  Email: {fake.email()}\")\n",
    "        print(f\"  Phone: {fake.phone_number()}\")\n",
    "        print(f\"  Address: {fake.address()}\")\n",
    "        print(f\"  Birthday: {fake.date_of_birth()}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"🏢 Business Information:\")\n",
    "    for i in range(3):\n",
    "        print(f\"  Company: {fake.company()}\")\n",
    "        print(f\"  Job: {fake.job()}\")\n",
    "        print(f\"  Department: {fake.bs()}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"💰 Financial Information:\")\n",
    "    for i in range(3):\n",
    "        print(f\"  Credit Card: {fake.credit_card_number()}\")\n",
    "        print(f\"  Bank Account: {fake.iban()}\")\n",
    "        print(f\"  Currency: {fake.currency_code()}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"✅ Faker generates realistic, random data every time!\")\n",
    "\n",
    "demonstrate_faker_basics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "380b0624-dcce-4496-bd19-d09f3d9bc305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_customer_data(num_customers=100):\n",
    "    \"\"\"\n",
    "    Generate realistic customer data using Faker\n",
    "    \"\"\"\n",
    "    print(f\"🎲 Generating {num_customers} realistic customers...\")\n",
    "    \n",
    "    customers = []\n",
    "    for i in range(num_customers):\n",
    "        customer = Row(\n",
    "            customer_id=f\"CUST{i+1:05d}\",\n",
    "            first_name=fake.first_name(),\n",
    "            last_name=fake.last_name(),\n",
    "            email=fake.email(),\n",
    "            phone=fake.phone_number(),\n",
    "            address=fake.address().replace('\\n', ', '),\n",
    "            city=fake.city(),\n",
    "            country=fake.country(),\n",
    "            date_of_birth=fake.date_of_birth(minimum_age=18, maximum_age=80),\n",
    "            registration_date=fake.date_between(start_date='-2y', end_date='today'),\n",
    "            account_status=fake.random_element(elements=('active', 'inactive', 'suspended')),\n",
    "            credit_score=fake.random_int(min=300, max=850),\n",
    "            annual_income=fake.random_int(min=25000, max=200000)\n",
    "        )\n",
    "        customers.append(customer)\n",
    "    \n",
    "    return spark.createDataFrame(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9698a81c-1b6d-4c39-ba73-15679022468e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_transaction_data(num_transactions=500, customer_ids=None):\n",
    "    \"\"\"\n",
    "    Generate realistic transaction data using Faker\n",
    "    \"\"\"\n",
    "    print(f\"🎲 Generating {num_transactions} realistic transactions...\")\n",
    "    \n",
    "    if customer_ids is None:\n",
    "        customer_ids = [f\"CUST{i+1:05d}\" for i in range(100)]\n",
    "    \n",
    "    transactions = []\n",
    "    for i in range(num_transactions):\n",
    "        transaction = Row(\n",
    "            transaction_id=f\"TXN{i+1:08d}\",\n",
    "            customer_id=fake.random_element(elements=customer_ids),\n",
    "            transaction_date=fake.date_between(start_date='-1y', end_date='today'),\n",
    "            transaction_time=fake.time(),\n",
    "            amount=round(fake.random.uniform(5.0, 2000.0), 2),\n",
    "            transaction_type=fake.random_element(elements=('purchase', 'refund', 'transfer')),\n",
    "            merchant_name=fake.company(),\n",
    "            category=fake.random_element(elements=('groceries', 'electronics', 'clothing', 'dining', 'travel', 'entertainment')),\n",
    "            payment_method=fake.random_element(elements=('credit_card', 'debit_card', 'bank_transfer', 'digital_wallet')),\n",
    "            status=fake.random_element(elements=('completed', 'pending', 'failed'))\n",
    "        )\n",
    "        transactions.append(transaction)\n",
    "    \n",
    "    return spark.createDataFrame(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca22c661-90df-4bfb-b72d-31e7b3399fa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_product_data(num_products=200):\n",
    "    \"\"\"\n",
    "    Generate realistic product catalog using Faker\n",
    "    \"\"\"\n",
    "    print(f\"🎲 Generating {num_products} realistic products...\")\n",
    "    \n",
    "    categories = ['Electronics', 'Clothing', 'Home & Garden', 'Books', 'Sports', 'Beauty']\n",
    "    \n",
    "    products = []\n",
    "    for i in range(num_products):\n",
    "        category = fake.random_element(elements=categories)\n",
    "        \n",
    "        # Generate category-specific product names\n",
    "        if category == 'Electronics':\n",
    "            product_name = fake.random_element(elements=['Smartphone', 'Laptop', 'Tablet', 'Headphones', 'TV', 'Camera'])\n",
    "        elif category == 'Clothing':\n",
    "            product_name = fake.random_element(elements=['T-Shirt', 'Jeans', 'Dress', 'Jacket', 'Shoes', 'Hat'])\n",
    "        else:\n",
    "            product_name = fake.word().title() + \" \" + fake.word().title()\n",
    "        \n",
    "        product = Row(\n",
    "            product_id=f\"PROD{i+1:05d}\",\n",
    "            product_name=product_name + \" \" + fake.color_name(),\n",
    "            category=category,\n",
    "            brand=fake.company(),\n",
    "            price=round(fake.random.uniform(9.99, 999.99), 2),\n",
    "            cost=round(fake.random.uniform(5.0, 500.0), 2),\n",
    "            stock_quantity=fake.random_int(min=0, max=1000),\n",
    "            supplier=fake.company(),\n",
    "            weight=round(fake.random.uniform(0.1, 50.0), 2),\n",
    "            dimensions=f\"{fake.random_int(1,50)}x{fake.random_int(1,50)}x{fake.random_int(1,50)} cm\",\n",
    "            rating=round(fake.random.uniform(1.0, 5.0), 1),\n",
    "            review_count=fake.random_int(min=0, max=5000)\n",
    "        )\n",
    "        products.append(product)\n",
    "    \n",
    "    return spark.createDataFrame(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cde486df-4950-430a-8705-508680c580c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n📊 Generating Sample Datasets:\n🎲 Generating 10 realistic customers...\n🎲 Generating 20 realistic transactions...\n🎲 Generating 15 realistic products...\n\n📋 Sample Customer Data:\n+-----------+----------+---------+--------------------+--------------------+--------------------+-----------------+---------+-------------+-----------------+--------------+------------+-------------+\n|customer_id|first_name|last_name|               email|               phone|             address|             city|  country|date_of_birth|registration_date|account_status|credit_score|annual_income|\n+-----------+----------+---------+--------------------+--------------------+--------------------+-----------------+---------+-------------+-----------------+--------------+------------+-------------+\n|  CUST00001|     Bryan|   Carter|danielmendez@exam...|  (680)746-4055x2076|94788 Anderson Tr...|   Sandersborough|Greenland|   1953-06-23|       2024-10-20|      inactive|         413|       181584|\n|  CUST00002|      Lisa|    Downs|pamelacollins@exa...|001-386-808-7651x217|425 Ellison Manor...|     Crystalshire|Mauritius|   1964-09-09|       2024-11-22|     suspended|         399|       117146|\n|  CUST00003|   Brandon|    Davis|danielroberts@exa...|        744-755-5600|366 Nicholas Esta...|Port Phillipville|  Myanmar|   1954-05-13|       2023-07-24|        active|         457|       163539|\n|  CUST00004|    Olivia|     Chen|keithbell@example...|  529.871.3111x42786|45180 Rachel Vill...|   Lake Jamesfort| Malaysia|   1954-05-23|       2023-07-15|        active|         661|        56553|\n|  CUST00005|  Lawrence|    Smith|kristenharding@ex...|        354.879.1218|311 Coleman Cresc...|  Sheppardchester|   Rwanda|   1952-03-08|       2024-11-22|        active|         667|       149623|\n+-----------+----------+---------+--------------------+--------------------+--------------------+-----------------+---------+-------------+-----------------+--------------+------------+-------------+\nonly showing top 5 rows\n\n\n📋 Sample Transaction Data:\n+--------------+-----------+----------------+----------------+------+----------------+--------------------+-------------+--------------+-------+\n|transaction_id|customer_id|transaction_date|transaction_time|amount|transaction_type|       merchant_name|     category|payment_method| status|\n+--------------+-----------+----------------+----------------+------+----------------+--------------------+-------------+--------------+-------+\n|   TXN00000001|  CUST00003|      2024-08-17|        22:19:50|624.55|        purchase|Lopez, Scott and ...|entertainment|digital_wallet|pending|\n|   TXN00000002|  CUST00010|      2025-02-06|        21:11:13|998.28|          refund|         Simpson PLC|    groceries|digital_wallet| failed|\n|   TXN00000003|  CUST00001|      2025-04-13|        08:09:00|870.62|        purchase|Flores, Thompson ...|  electronics|digital_wallet|pending|\n|   TXN00000004|  CUST00007|      2024-09-10|        13:18:17|1490.3|          refund|Jones, Foster and...|       travel|   credit_card| failed|\n|   TXN00000005|  CUST00003|      2024-08-19|        07:23:04|404.15|        purchase|     Henderson-Mccoy|entertainment|    debit_card| failed|\n+--------------+-----------+----------------+----------------+------+----------------+--------------------+-------------+--------------+-------+\nonly showing top 5 rows\n\n\n📋 Sample Product Data:\n+----------+--------------------+-----------+--------------------+------+------+--------------+--------------------+------+-----------+------+------------+\n|product_id|        product_name|   category|               brand| price|  cost|stock_quantity|            supplier|weight| dimensions|rating|review_count|\n+----------+--------------------+-----------+--------------------+------+------+--------------+--------------------+------+-----------+------+------------+\n| PROD00001|  Pattern State Gray|     Beauty|       Wilkins-Jones|953.66| 44.18|           597|Wolfe, Hawkins an...|  0.69|40x39x30 cm|   3.4|        1456|\n| PROD00002|     Tablet SeaGreen|Electronics|   Thompson and Sons|105.06|371.96|           526|          Hodges PLC|  8.87| 32x2x36 cm|   4.5|        3019|\n| PROD00003|Pm Kind DarkSlate...|      Books|        Jackson-Webb|161.27|181.52|           422|        Smith-Peters|  1.75|  6x34x1 cm|   3.7|        3014|\n| PROD00004|Purpose Section S...|     Beauty|Miranda, Foster a...| 514.6|232.68|            86|Cole, Thompson an...| 14.35|41x37x27 cm|   4.5|        2202|\n| PROD00005|   Camera GhostWhite|Electronics|          Miller Ltd|787.95|470.54|           559|White, Maynard an...| 24.35| 4x40x50 cm|   1.9|        2115|\n+----------+--------------------+-----------+--------------------+------+------+--------------+--------------------+------+-----------+------+------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Generate sample datasets\n",
    "print(\"\\n📊 Generating Sample Datasets:\")\n",
    "sample_customers = generate_customer_data(10)\n",
    "sample_transactions = generate_transaction_data(20, [f\"CUST{i+1:05d}\" for i in range(10)])\n",
    "sample_products = generate_product_data(15)\n",
    "\n",
    "print(\"\\n📋 Sample Customer Data:\")\n",
    "sample_customers.show(5)\n",
    "\n",
    "print(\"\\n📋 Sample Transaction Data:\")\n",
    "sample_transactions.show(5)\n",
    "\n",
    "print(\"\\n📋 Sample Product Data:\")\n",
    "sample_products.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fefc2fa4-51d6-42d8-bef5-d9c23e74e529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_customer_data_from_source(num_customers=1000):\n",
    "    \"\"\"\n",
    "    EXTRACTION: Simulate extracting customer data from external system\n",
    "    \"\"\"\n",
    "    print(\"📥 STEP 1: Extracting Customer Data\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Simulate data extraction from external CRM system\n",
    "        print(\"🔄 Connecting to CRM system...\")\n",
    "        print(\"🔄 Executing customer data query...\")\n",
    "        \n",
    "        customers_df = generate_customer_data(num_customers)\n",
    "        \n",
    "        print(f\"✅ Successfully extracted {customers_df.count()} customer records\")\n",
    "        print(\"📊 Sample extracted data:\")\n",
    "        customers_df.show(3)\n",
    "        \n",
    "        return customers_df, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Customer extraction failed: {e}\")\n",
    "        return None, False\n",
    "\n",
    "def extract_transaction_data_from_source(customer_ids, num_transactions=5000):\n",
    "    \"\"\"\n",
    "    EXTRACTION: Simulate extracting transaction data from external system\n",
    "    \"\"\"\n",
    "    print(\"\\n📥 STEP 2: Extracting Transaction Data\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Simulate data extraction from external transaction system\n",
    "        print(\"🔄 Connecting to transaction database...\")\n",
    "        print(\"🔄 Executing transaction data query...\")\n",
    "        \n",
    "        transactions_df = generate_transaction_data(num_transactions, customer_ids)\n",
    "        \n",
    "        print(f\"✅ Successfully extracted {transactions_df.count()} transaction records\")\n",
    "        print(\"📊 Sample extracted data:\")\n",
    "        transactions_df.show(3)\n",
    "        \n",
    "        return transactions_df, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Transaction extraction failed: {e}\")\n",
    "        return None, False\n",
    "\n",
    "def extract_product_data_from_source(num_products=1000):\n",
    "    \"\"\"\n",
    "    EXTRACTION: Simulate extracting product data from external system\n",
    "    \"\"\"\n",
    "    print(\"\\n📥 STEP 3: Extracting Product Data\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Simulate data extraction from external product catalog\n",
    "        print(\"🔄 Connecting to product catalog system...\")\n",
    "        print(\"🔄 Executing product data query...\")\n",
    "        \n",
    "        products_df = generate_product_data(num_products)\n",
    "        \n",
    "        print(f\"✅ Successfully extracted {products_df.count()} product records\")\n",
    "        print(\"📊 Sample extracted data:\")\n",
    "        products_df.show(3)\n",
    "        \n",
    "        return products_df, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Product extraction failed: {e}\")\n",
    "        return None, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c0b693-9b2b-441b-8df4-29a9a52bc303",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform_customer_data(customers_df):\n",
    "    \"\"\"\n",
    "    TRANSFORMATION: Clean and enrich customer data\n",
    "    \"\"\"\n",
    "    print(\"⚙️ STEP 1: Transforming Customer Data\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        from pyspark.sql.functions import upper, lower, regexp_replace, concat, lit, current_date, datediff\n",
    "        \n",
    "        # Data cleaning and standardization\n",
    "        cleaned_customers = customers_df.withColumn(\n",
    "            \"full_name\", concat(col(\"first_name\"), lit(\" \"), col(\"last_name\"))\n",
    "        ).withColumn(\n",
    "            \"email_clean\", lower(col(\"email\"))\n",
    "        ).withColumn(\n",
    "            \"phone_clean\", regexp_replace(col(\"phone\"), \"[^0-9]\", \"\")\n",
    "        ).withColumn(\n",
    "            \"age\", (datediff(current_date(), col(\"date_of_birth\")) / 365.25).cast(\"int\")\n",
    "        )\n",
    "        \n",
    "        # Customer segmentation based on credit score and income\n",
    "        segmented_customers = cleaned_customers.withColumn(\n",
    "            \"customer_segment\",\n",
    "            when((col(\"credit_score\") >= 750) & (col(\"annual_income\") >= 75000), \"Premium\")\n",
    "            .when((col(\"credit_score\") >= 650) & (col(\"annual_income\") >= 50000), \"Standard\")\n",
    "            .when(col(\"credit_score\") >= 550, \"Basic\")\n",
    "            .otherwise(\"Risk\")\n",
    "        ).withColumn(\n",
    "            \"risk_category\",\n",
    "            when(col(\"credit_score\") >= 750, \"Low Risk\")\n",
    "            .when(col(\"credit_score\") >= 650, \"Medium Risk\")\n",
    "            .otherwise(\"High Risk\")\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Transformed {segmented_customers.count()} customer records\")\n",
    "        print(\"📊 Customer segmentation summary:\")\n",
    "        segmented_customers.groupBy(\"customer_segment\").count().show()\n",
    "        \n",
    "        return segmented_customers, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Customer transformation failed: {e}\")\n",
    "        return customers_df, False\n",
    "\n",
    "def transform_transaction_data(transactions_df):\n",
    "    \"\"\"\n",
    "    TRANSFORMATION: Process and aggregate transaction data\n",
    "    \"\"\"\n",
    "    print(\"\\n⚙️ STEP 2: Transforming Transaction Data\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        from pyspark.sql.functions import year, month, dayofweek, hour, when\n",
    "        \n",
    "        # Add time-based features\n",
    "        enhanced_transactions = transactions_df.withColumn(\n",
    "            \"transaction_year\", year(col(\"transaction_date\"))\n",
    "        ).withColumn(\n",
    "            \"transaction_month\", month(col(\"transaction_date\"))\n",
    "        ).withColumn(\n",
    "            \"day_of_week\", dayofweek(col(\"transaction_date\"))\n",
    "        ).withColumn(\n",
    "            \"is_weekend\", when(col(\"day_of_week\").isin([1, 7]), True).otherwise(False)\n",
    "        )\n",
    "        \n",
    "        # Categorize transaction amounts\n",
    "        categorized_transactions = enhanced_transactions.withColumn(\n",
    "            \"amount_category\",\n",
    "            when(col(\"amount\") >= 500, \"High Value\")\n",
    "            .when(col(\"amount\") >= 100, \"Medium Value\")\n",
    "            .when(col(\"amount\") >= 20, \"Low Value\")\n",
    "            .otherwise(\"Micro Transaction\")\n",
    "        )\n",
    "        \n",
    "        # Add business logic flags\n",
    "        final_transactions = categorized_transactions.withColumn(\n",
    "            \"is_large_transaction\", col(\"amount\") >= 1000\n",
    "        ).withColumn(\n",
    "            \"requires_review\", \n",
    "            (col(\"amount\") >= 1000) | (col(\"transaction_type\") == \"refund\")\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Transformed {final_transactions.count()} transaction records\")\n",
    "        print(\"📊 Transaction amount distribution:\")\n",
    "        final_transactions.groupBy(\"amount_category\").count().show()\n",
    "        \n",
    "        return final_transactions, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Transaction transformation failed: {e}\")\n",
    "        return transactions_df, False\n",
    "\n",
    "def create_customer_analytics(customers_df, transactions_df):\n",
    "    \"\"\"\n",
    "    TRANSFORMATION: Create customer analytics by joining customer and transaction data\n",
    "    \"\"\"\n",
    "    print(\"\\n⚙️ STEP 3: Creating Customer Analytics\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Calculate customer transaction metrics\n",
    "        customer_metrics = transactions_df.filter(\n",
    "            col(\"status\") == \"completed\"\n",
    "        ).groupBy(\"customer_id\").agg(\n",
    "            spark_sum(\"amount\").alias(\"total_spent\"),\n",
    "            count(\"transaction_id\").alias(\"transaction_count\"),\n",
    "            avg(\"amount\").alias(\"avg_transaction_amount\"),\n",
    "            spark_max(\"amount\").alias(\"max_transaction_amount\"),\n",
    "            count(when(col(\"transaction_type\") == \"refund\", 1)).alias(\"refund_count\")\n",
    "        )\n",
    "        \n",
    "        # Join customer data with transaction metrics\n",
    "        customer_analytics = customers_df.join(\n",
    "            customer_metrics, \"customer_id\", \"left\"\n",
    "        ).fillna({\n",
    "            \"total_spent\": 0.0,\n",
    "            \"transaction_count\": 0,\n",
    "            \"avg_transaction_amount\": 0.0,\n",
    "            \"max_transaction_amount\": 0.0,\n",
    "            \"refund_count\": 0\n",
    "        })\n",
    "        \n",
    "        # Add customer value scoring\n",
    "        customer_analytics = customer_analytics.withColumn(\n",
    "            \"customer_value_score\",\n",
    "            (col(\"total_spent\") * 0.4) + \n",
    "            (col(\"transaction_count\") * 10) + \n",
    "            (col(\"credit_score\") * 0.1) - \n",
    "            (col(\"refund_count\") * 50)\n",
    "        ).withColumn(\n",
    "            \"customer_tier\",\n",
    "            when(col(\"customer_value_score\") >= 1000, \"Platinum\")\n",
    "            .when(col(\"customer_value_score\") >= 500, \"Gold\")\n",
    "            .when(col(\"customer_value_score\") >= 200, \"Silver\")\n",
    "            .otherwise(\"Bronze\")\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Created analytics for {customer_analytics.count()} customers\")\n",
    "        print(\"📊 Customer tier distribution:\")\n",
    "        customer_analytics.groupBy(\"customer_tier\").count().show()\n",
    "        \n",
    "        return customer_analytics, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Customer analytics creation failed: {e}\")\n",
    "        return customers_df, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54005da7-f621-4b71-a404-a000e184600e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_to_data_warehouse(data_df, table_name, warehouse_connection):\n",
    "    \"\"\"\n",
    "    LOADING: Save processed data to data warehouse (mocked)\n",
    "    \"\"\"\n",
    "    print(f\"💾 Loading data to warehouse table: {table_name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Convert DataFrame to records for warehouse loading\n",
    "        record_count = data_df.count()\n",
    "        \n",
    "        # Simulate data validation before loading\n",
    "        print(\"🔍 Validating data quality...\")\n",
    "        \n",
    "        # Check for required fields\n",
    "        null_ids = data_df.filter(col(\"customer_id\").isNull()).count()\n",
    "        if null_ids > 0:\n",
    "            raise Exception(f\"Found {null_ids} records with null customer_id\")\n",
    "        \n",
    "        # Check for duplicate records\n",
    "        total_records = data_df.count()\n",
    "        unique_records = data_df.distinct().count()\n",
    "        if total_records != unique_records:\n",
    "            print(f\"⚠️ Warning: Found {total_records - unique_records} duplicate records\")\n",
    "        \n",
    "        # Mock warehouse operations\n",
    "        print(\"🔄 Connecting to data warehouse...\")\n",
    "        warehouse_connection.create_table_if_not_exists(table_name)\n",
    "        \n",
    "        print(\"🔄 Inserting records...\")\n",
    "        warehouse_connection.insert_batch(table_name, record_count)\n",
    "        \n",
    "        print(\"🔄 Creating indexes...\")\n",
    "        warehouse_connection.create_indexes(table_name)\n",
    "        \n",
    "        print(\"🔄 Committing transaction...\")\n",
    "        warehouse_connection.commit()\n",
    "        \n",
    "        print(f\"✅ Successfully loaded {record_count} records to {table_name}\")\n",
    "        return record_count, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Loading to {table_name} failed: {e}\")\n",
    "        warehouse_connection.rollback()\n",
    "        return 0, False\n",
    "\n",
    "def load_to_analytics_store(analytics_df, analytics_connection):\n",
    "    \"\"\"\n",
    "    LOADING: Save analytics data to analytics store (mocked)\n",
    "    \"\"\"\n",
    "    print(\"💾 Loading data to analytics store\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        record_count = analytics_df.count()\n",
    "        \n",
    "        # Mock analytics store operations\n",
    "        print(\"🔄 Connecting to analytics store...\")\n",
    "        analytics_connection.prepare_analytics_tables()\n",
    "        \n",
    "        print(\"🔄 Loading customer analytics...\")\n",
    "        analytics_connection.load_customer_metrics(record_count)\n",
    "        \n",
    "        print(\"🔄 Updating dashboards...\")\n",
    "        analytics_connection.refresh_dashboards()\n",
    "        \n",
    "        print(f\"✅ Successfully loaded {record_count} analytics records\")\n",
    "        return record_count, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Analytics loading failed: {e}\")\n",
    "        return 0, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73311980-8a9c-47e4-900d-c1e086d00e70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_complete_e2e_pipeline(num_customers=1000, num_transactions=5000):\n",
    "    \"\"\"\n",
    "    Complete end-to-end pipeline: Extract → Transform → Load\n",
    "    \"\"\"\n",
    "    print(\"🚀 Starting Complete E2E Data Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    pipeline_results = {\n",
    "        \"extraction_success\": False,\n",
    "        \"transformation_success\": False,\n",
    "        \"loading_success\": False,\n",
    "        \"total_customers\": 0,\n",
    "        \"total_transactions\": 0,\n",
    "        \"analytics_records\": 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # STEP 1: EXTRACTION\n",
    "        print(\"\\n📥 PHASE 1: DATA EXTRACTION\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        # Extract customers\n",
    "        customers_df, customer_extract_success = extract_customer_data_from_source(num_customers)\n",
    "        if not customer_extract_success:\n",
    "            raise Exception(\"Customer extraction failed\")\n",
    "        \n",
    "        # Get customer IDs for transaction extraction\n",
    "        customer_ids = [row.customer_id for row in customers_df.select(\"customer_id\").collect()]\n",
    "        \n",
    "        # Extract transactions\n",
    "        transactions_df, transaction_extract_success = extract_transaction_data_from_source(\n",
    "            customer_ids, num_transactions\n",
    "        )\n",
    "        if not transaction_extract_success:\n",
    "            raise Exception(\"Transaction extraction failed\")\n",
    "        \n",
    "        pipeline_results[\"extraction_success\"] = True\n",
    "        pipeline_results[\"total_customers\"] = customers_df.count()\n",
    "        pipeline_results[\"total_transactions\"] = transactions_df.count()\n",
    "        \n",
    "        # STEP 2: TRANSFORMATION\n",
    "        print(\"\\n⚙️ PHASE 2: DATA TRANSFORMATION\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        # Transform customer data\n",
    "        transformed_customers, customer_transform_success = transform_customer_data(customers_df)\n",
    "        if not customer_transform_success:\n",
    "            raise Exception(\"Customer transformation failed\")\n",
    "        \n",
    "        # Transform transaction data\n",
    "        transformed_transactions, transaction_transform_success = transform_transaction_data(transactions_df)\n",
    "        if not transaction_transform_success:\n",
    "            raise Exception(\"Transaction transformation failed\")\n",
    "        \n",
    "        # Create customer analytics\n",
    "        customer_analytics, analytics_success = create_customer_analytics(\n",
    "            transformed_customers, transformed_transactions\n",
    "        )\n",
    "        if not analytics_success:\n",
    "            raise Exception(\"Analytics creation failed\")\n",
    "        \n",
    "        pipeline_results[\"transformation_success\"] = True\n",
    "        pipeline_results[\"analytics_records\"] = customer_analytics.count()\n",
    "        \n",
    "        # STEP 3: LOADING (with mocks)\n",
    "        print(\"\\n💾 PHASE 3: DATA LOADING\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        # Create mock connections\n",
    "        mock_warehouse = MagicMock()\n",
    "        mock_warehouse.create_table_if_not_exists.return_value = None\n",
    "        mock_warehouse.insert_batch.return_value = None\n",
    "        mock_warehouse.create_indexes.return_value = None\n",
    "        mock_warehouse.commit.return_value = None\n",
    "        mock_warehouse.rollback.return_value = None\n",
    "        \n",
    "        mock_analytics = MagicMock()\n",
    "        mock_analytics.prepare_analytics_tables.return_value = None\n",
    "        mock_analytics.load_customer_metrics.return_value = None\n",
    "        mock_analytics.refresh_dashboards.return_value = None\n",
    "        \n",
    "        # Load data\n",
    "        customer_load_count, customer_load_success = load_to_data_warehouse(\n",
    "            transformed_customers, \"customers\", mock_warehouse\n",
    "        )\n",
    "        \n",
    "        transaction_load_count, transaction_load_success = load_to_data_warehouse(\n",
    "            transformed_transactions, \"transactions\", mock_warehouse\n",
    "        )\n",
    "        \n",
    "        analytics_load_count, analytics_load_success = load_to_analytics_store(\n",
    "            customer_analytics, mock_analytics\n",
    "        )\n",
    "        \n",
    "        pipeline_results[\"loading_success\"] = (\n",
    "            customer_load_success and transaction_load_success and analytics_load_success\n",
    "        )\n",
    "        \n",
    "        # PIPELINE SUMMARY\n",
    "        print(\"\\n📊 PIPELINE EXECUTION SUMMARY\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        all_phases_success = (\n",
    "            pipeline_results[\"extraction_success\"] and \n",
    "            pipeline_results[\"transformation_success\"] and \n",
    "            pipeline_results[\"loading_success\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"📥 Extraction: {'✅ SUCCESS' if pipeline_results['extraction_success'] else '❌ FAILED'}\")\n",
    "        print(f\"⚙️ Transformation: {'✅ SUCCESS' if pipeline_results['transformation_success'] else '❌ FAILED'}\")\n",
    "        print(f\"💾 Loading: {'✅ SUCCESS' if pipeline_results['loading_success'] else '❌ FAILED'}\")\n",
    "        print(f\"🎯 Overall Pipeline: {'✅ SUCCESS' if all_phases_success else '❌ FAILED'}\")\n",
    "        \n",
    "        print(f\"\\n📊 Data Processed:\")\n",
    "        print(f\"  👥 Customers: {pipeline_results['total_customers']}\")\n",
    "        print(f\"  💳 Transactions: {pipeline_results['total_transactions']}\")\n",
    "        print(f\"  📈 Analytics Records: {pipeline_results['analytics_records']}\")\n",
    "        \n",
    "        return pipeline_results, customer_analytics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ PIPELINE FAILED: {e}\")\n",
    "        return pipeline_results, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dcfde3a-7821-47f4-b3d7-19260bdd909b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n🏆 PART 8: COMPLETE INTEGRATION TEST SUITE\n============================================================\n🧪 Running Complete Integration Test Suite\n==================================================\n\n🚀 Starting Integration Test Execution...\n🧪 INTEGRATION TEST 1: Small Dataset\n----------------------------------------\n🚀 Starting Complete E2E Data Pipeline\n==================================================\n\n📥 PHASE 1: DATA EXTRACTION\n==============================\n📥 STEP 1: Extracting Customer Data\n----------------------------------------\n🔄 Connecting to CRM system...\n🔄 Executing customer data query...\n🎲 Generating 50 realistic customers...\n✅ Successfully extracted 50 customer records\n📊 Sample extracted data:\n+-----------+----------+---------+--------------------+--------------------+--------------------+------------+----------+-------------+-----------------+--------------+------------+-------------+\n|customer_id|first_name|last_name|               email|               phone|             address|        city|   country|date_of_birth|registration_date|account_status|credit_score|annual_income|\n+-----------+----------+---------+--------------------+--------------------+--------------------+------------+----------+-------------+-----------------+--------------+------------+-------------+\n|  CUST00001|     Tammy|Mcfarland|hughesmichael@exa...|001-677-741-0432x...|426 Julie Terrace...|   Reneeland|  Ethiopia|   1945-08-07|       2024-08-13|        active|         397|        81805|\n|  CUST00002|     Stacy|   French|justinramsey@exam...|        859-952-9573|9491 Lonnie Islan...|   Port Mark|Azerbaijan|   1949-10-27|       2024-02-01|        active|         325|       185275|\n|  CUST00003|      Erik|   Wright|  emma57@example.org|       (781)723-6017|3929 Burch Divide...|Stefanieview| Venezuela|   1990-06-01|       2024-10-11|      inactive|         560|       168945|\n+-----------+----------+---------+--------------------+--------------------+--------------------+------------+----------+-------------+-----------------+--------------+------------+-------------+\nonly showing top 3 rows\n\n\n📥 STEP 2: Extracting Transaction Data\n----------------------------------------\n🔄 Connecting to transaction database...\n🔄 Executing transaction data query...\n🎲 Generating 200 realistic transactions...\n✅ Successfully extracted 200 transaction records\n📊 Sample extracted data:\n+--------------+-----------+----------------+----------------+------+----------------+--------------------+-------------+--------------+---------+\n|transaction_id|customer_id|transaction_date|transaction_time|amount|transaction_type|       merchant_name|     category|payment_method|   status|\n+--------------+-----------+----------------+----------------+------+----------------+--------------------+-------------+--------------+---------+\n|   TXN00000001|  CUST00005|      2025-03-18|        15:00:29|460.38|          refund|        Shepherd LLC|       travel|   credit_card|  pending|\n|   TXN00000002|  CUST00035|      2024-06-23|        02:46:17| 91.87|        transfer|Kent, Neal and Wa...|       dining|   credit_card|  pending|\n|   TXN00000003|  CUST00032|      2024-11-09|        00:39:30|279.22|        transfer|          Patton PLC|entertainment| bank_transfer|completed|\n+--------------+-----------+----------------+----------------+------+----------------+--------------------+-------------+--------------+---------+\nonly showing top 3 rows\n\n\n⚙️ PHASE 2: DATA TRANSFORMATION\n==============================\n⚙️ STEP 1: Transforming Customer Data\n----------------------------------------\n✅ Transformed 50 customer records\n📊 Customer segmentation summary:\n+----------------+-----+\n|customer_segment|count|\n+----------------+-----+\n|           Basic|   16|\n|            Risk|   24|\n|        Standard|    7|\n|         Premium|    3|\n+----------------+-----+\n\n\n⚙️ STEP 2: Transforming Transaction Data\n----------------------------------------\n✅ Transformed 200 transaction records\n📊 Transaction amount distribution:\n+-----------------+-----+\n|  amount_category|count|\n+-----------------+-----+\n|     Medium Value|   34|\n|       High Value|  148|\n|Micro Transaction|    6|\n|        Low Value|   12|\n+-----------------+-----+\n\n\n⚙️ STEP 3: Creating Customer Analytics\n----------------------------------------\n✅ Created analytics for 50 customers\n📊 Customer tier distribution:\n+-------------+-----+\n|customer_tier|count|\n+-------------+-----+\n|     Platinum|    5|\n|       Silver|   11|\n|         Gold|   17|\n|       Bronze|   17|\n+-------------+-----+\n\n\n💾 PHASE 3: DATA LOADING\n==============================\n💾 Loading data to warehouse table: customers\n----------------------------------------\n🔍 Validating data quality...\n🔄 Connecting to data warehouse...\n🔄 Inserting records...\n🔄 Creating indexes...\n🔄 Committing transaction...\n✅ Successfully loaded 50 records to customers\n💾 Loading data to warehouse table: transactions\n----------------------------------------\n🔍 Validating data quality...\n🔄 Connecting to data warehouse...\n🔄 Inserting records...\n🔄 Creating indexes...\n🔄 Committing transaction...\n✅ Successfully loaded 200 records to transactions\n💾 Loading data to analytics store\n----------------------------------------\n🔄 Connecting to analytics store...\n🔄 Loading customer analytics...\n🔄 Updating dashboards...\n✅ Successfully loaded 50 analytics records\n\n📊 PIPELINE EXECUTION SUMMARY\n========================================\n📥 Extraction: ✅ SUCCESS\n⚙️ Transformation: ✅ SUCCESS\n💾 Loading: ✅ SUCCESS\n🎯 Overall Pipeline: ✅ SUCCESS\n\n📊 Data Processed:\n  👥 Customers: 50\n  💳 Transactions: 200\n  📈 Analytics Records: 50\n\n🎯 Small Dataset Test: ✅ PASSED\n\n🧪 INTEGRATION TEST 2: Large Dataset\n----------------------------------------\n🚀 Starting Complete E2E Data Pipeline\n==================================================\n\n📥 PHASE 1: DATA EXTRACTION\n==============================\n📥 STEP 1: Extracting Customer Data\n----------------------------------------\n🔄 Connecting to CRM system...\n🔄 Executing customer data query...\n🎲 Generating 2000 realistic customers...\n✅ Successfully extracted 2000 customer records\n📊 Sample extracted data:\n+-----------+----------+---------+--------------------+------------------+--------------------+-------------------+-----------+-------------+-----------------+--------------+------------+-------------+\n|customer_id|first_name|last_name|               email|             phone|             address|               city|    country|date_of_birth|registration_date|account_status|credit_score|annual_income|\n+-----------+----------+---------+--------------------+------------------+--------------------+-------------------+-----------+-------------+-----------------+--------------+------------+-------------+\n|  CUST00001|      Troy|   Miller|leechristy@exampl...|        8032304005|Unit 4832 Box 634...|         Jonesmouth|  Sri Lanka|   1965-10-04|       2024-03-26|      inactive|         440|        57199|\n|  CUST00002|    Connie|   Walker|   ybyrd@example.com|276-460-6034x86777|26046 April Forks...|  New Thomaschester|   Honduras|   1976-04-14|       2024-02-11|     suspended|         425|       169248|\n|  CUST00003|   Michael| Phillips|phelpscatherine@e...| 228-588-5695x0466|800 Pamela Lodge ...|North Margaretmouth|Saint Lucia|   1958-09-02|       2025-04-16|      inactive|         553|       135411|\n+-----------+----------+---------+--------------------+------------------+--------------------+-------------------+-----------+-------------+-----------------+--------------+------------+-------------+\nonly showing top 3 rows\n\n\n📥 STEP 2: Extracting Transaction Data\n----------------------------------------\n🔄 Connecting to transaction database...\n🔄 Executing transaction data query...\n🎲 Generating 10000 realistic transactions...\n✅ Successfully extracted 10000 transaction records\n📊 Sample extracted data:\n+--------------+-----------+----------------+----------------+-------+----------------+---------------+---------+--------------+---------+\n|transaction_id|customer_id|transaction_date|transaction_time| amount|transaction_type|  merchant_name| category|payment_method|   status|\n+--------------+-----------+----------------+----------------+-------+----------------+---------------+---------+--------------+---------+\n|   TXN00000001|  CUST01385|      2024-12-21|        12:28:23|1751.49|        purchase|      Page-West|groceries|digital_wallet|  pending|\n|   TXN00000002|  CUST00534|      2024-12-03|        03:31:25| 167.49|          refund|   Santiago LLC|   travel|   credit_card|   failed|\n|   TXN00000003|  CUST00838|      2025-03-14|        19:29:40|1344.18|        transfer|Patterson Group|   travel|digital_wallet|completed|\n+--------------+-----------+----------------+----------------+-------+----------------+---------------+---------+--------------+---------+\nonly showing top 3 rows\n\n\n⚙️ PHASE 2: DATA TRANSFORMATION\n==============================\n⚙️ STEP 1: Transforming Customer Data\n----------------------------------------\n✅ Transformed 2000 customer records\n📊 Customer segmentation summary:\n+----------------+-----+\n|customer_segment|count|\n+----------------+-----+\n|         Premium|  251|\n|           Basic|  453|\n|            Risk|  904|\n|        Standard|  392|\n+----------------+-----+\n\n\n⚙️ STEP 2: Transforming Transaction Data\n----------------------------------------\n✅ Transformed 10000 transaction records\n📊 Transaction amount distribution:\n+-----------------+-----+\n|  amount_category|count|\n+-----------------+-----+\n|     Medium Value| 2007|\n|       High Value| 7496|\n|Micro Transaction|   66|\n|        Low Value|  431|\n+-----------------+-----+\n\n\n⚙️ STEP 3: Creating Customer Analytics\n----------------------------------------\n✅ Created analytics for 2000 customers\n📊 Customer tier distribution:\n+-------------+-----+\n|customer_tier|count|\n+-------------+-----+\n|     Platinum|  533|\n|       Silver|  305|\n|         Gold|  640|\n|       Bronze|  522|\n+-------------+-----+\n\n\n💾 PHASE 3: DATA LOADING\n==============================\n💾 Loading data to warehouse table: customers\n----------------------------------------\n🔍 Validating data quality...\n🔄 Connecting to data warehouse...\n🔄 Inserting records...\n🔄 Creating indexes...\n🔄 Committing transaction...\n✅ Successfully loaded 2000 records to customers\n💾 Loading data to warehouse table: transactions\n----------------------------------------\n🔍 Validating data quality...\n🔄 Connecting to data warehouse...\n🔄 Inserting records...\n🔄 Creating indexes...\n🔄 Committing transaction...\n✅ Successfully loaded 10000 records to transactions\n💾 Loading data to analytics store\n----------------------------------------\n🔄 Connecting to analytics store...\n🔄 Loading customer analytics...\n🔄 Updating dashboards...\n✅ Successfully loaded 2000 analytics records\n\n📊 PIPELINE EXECUTION SUMMARY\n========================================\n📥 Extraction: ✅ SUCCESS\n⚙️ Transformation: ✅ SUCCESS\n💾 Loading: ✅ SUCCESS\n🎯 Overall Pipeline: ✅ SUCCESS\n\n📊 Data Processed:\n  👥 Customers: 2000\n  💳 Transactions: 10000\n  📈 Analytics Records: 2000\n\n⏱️ Execution Time: 14.48 seconds\n🎯 Large Dataset Test: ✅ PASSED\n\n🧪 INTEGRATION TEST 3: Data Quality Validation\n----------------------------------------\n🚀 Starting Complete E2E Data Pipeline\n==================================================\n\n📥 PHASE 1: DATA EXTRACTION\n==============================\n📥 STEP 1: Extracting Customer Data\n----------------------------------------\n🔄 Connecting to CRM system...\n🔄 Executing customer data query...\n🎲 Generating 500 realistic customers...\n✅ Successfully extracted 500 customer records\n📊 Sample extracted data:\n+-----------+----------+---------+--------------------+-------------------+--------------------+-------------+---------+-------------+-----------------+--------------+------------+-------------+\n|customer_id|first_name|last_name|               email|              phone|             address|         city|  country|date_of_birth|registration_date|account_status|credit_score|annual_income|\n+-----------+----------+---------+--------------------+-------------------+--------------------+-------------+---------+-------------+-----------------+--------------+------------+-------------+\n|  CUST00001|      Lisa|   Murphy|garrett56@example...|+1-421-534-9755x821|USS Graham, FPO A...|West Davidton|Hong Kong|   2000-09-13|       2025-04-26|        active|         635|       155910|\n|  CUST00002|     Wayne|   Taylor|ljohnson@example.com|+1-888-348-7018x433|76873 Jose Wall, ...|   South Lisa|    Chile|   1995-07-11|       2024-03-31|        active|         544|       178635|\n|  CUST00003|     Craig|   Nelson|gloriapearson@exa...| 659-791-6275x83221|24047 Jennifer Fl...|  Ashleyshire|  Bermuda|   1996-12-16|       2023-12-28|      inactive|         629|       121199|\n+-----------+----------+---------+--------------------+-------------------+--------------------+-------------+---------+-------------+-----------------+--------------+------------+-------------+\nonly showing top 3 rows\n\n\n📥 STEP 2: Extracting Transaction Data\n----------------------------------------\n🔄 Connecting to transaction database...\n🔄 Executing transaction data query...\n🎲 Generating 2000 realistic transactions...\n✅ Successfully extracted 2000 transaction records\n📊 Sample extracted data:\n+--------------+-----------+----------------+----------------+-------+----------------+--------------------+-----------+--------------+---------+\n|transaction_id|customer_id|transaction_date|transaction_time| amount|transaction_type|       merchant_name|   category|payment_method|   status|\n+--------------+-----------+----------------+----------------+-------+----------------+--------------------+-----------+--------------+---------+\n|   TXN00000001|  CUST00021|      2024-11-23|        23:01:06|1630.17|          refund|            Knox LLC|     travel|    debit_card|completed|\n|   TXN00000002|  CUST00381|      2024-12-30|        14:25:26|  56.66|        transfer|Silva, Bautista a...|electronics|    debit_card|   failed|\n|   TXN00000003|  CUST00106|      2024-09-21|        08:39:28|1942.46|        transfer|        Wilson Group|  groceries|   credit_card|   failed|\n+--------------+-----------+----------------+----------------+-------+----------------+--------------------+-----------+--------------+---------+\nonly showing top 3 rows\n\n\n⚙️ PHASE 2: DATA TRANSFORMATION\n==============================\n⚙️ STEP 1: Transforming Customer Data\n----------------------------------------\n✅ Transformed 500 customer records\n📊 Customer segmentation summary:\n+----------------+-----+\n|customer_segment|count|\n+----------------+-----+\n|         Premium|   66|\n|           Basic|  126|\n|            Risk|  228|\n|        Standard|   80|\n+----------------+-----+\n\n\n⚙️ STEP 2: Transforming Transaction Data\n----------------------------------------\n✅ Transformed 2000 transaction records\n📊 Transaction amount distribution:\n+-----------------+-----+\n|  amount_category|count|\n+-----------------+-----+\n|     Medium Value|  409|\n|       High Value| 1486|\n|Micro Transaction|   18|\n|        Low Value|   87|\n+-----------------+-----+\n\n\n⚙️ STEP 3: Creating Customer Analytics\n----------------------------------------\n✅ Created analytics for 500 customers\n📊 Customer tier distribution:\n+-------------+-----+\n|customer_tier|count|\n+-------------+-----+\n|     Platinum|   83|\n|       Silver|   84|\n|         Gold|  166|\n|       Bronze|  167|\n+-------------+-----+\n\n\n💾 PHASE 3: DATA LOADING\n==============================\n💾 Loading data to warehouse table: customers\n----------------------------------------\n🔍 Validating data quality...\n🔄 Connecting to data warehouse...\n🔄 Inserting records...\n🔄 Creating indexes...\n🔄 Committing transaction...\n✅ Successfully loaded 500 records to customers\n💾 Loading data to warehouse table: transactions\n----------------------------------------\n🔍 Validating data quality...\n🔄 Connecting to data warehouse...\n🔄 Inserting records...\n🔄 Creating indexes...\n🔄 Committing transaction...\n✅ Successfully loaded 2000 records to transactions\n💾 Loading data to analytics store\n----------------------------------------\n🔄 Connecting to analytics store...\n🔄 Loading customer analytics...\n🔄 Updating dashboards...\n✅ Successfully loaded 500 analytics records\n\n📊 PIPELINE EXECUTION SUMMARY\n========================================\n📥 Extraction: ✅ SUCCESS\n⚙️ Transformation: ✅ SUCCESS\n💾 Loading: ✅ SUCCESS\n🎯 Overall Pipeline: ✅ SUCCESS\n\n📊 Data Processed:\n  👥 Customers: 500\n  💳 Transactions: 2000\n  📈 Analytics Records: 500\n\n🔍 Data Quality Results:\n  no_null_ids: ✅ PASSED\n  valid_emails: ✅ PASSED\n  valid_credit_scores: ✅ PASSED\n  segments_assigned: ✅ PASSED\n  reasonable_ages: ✅ PASSED\n\n🎯 Data Quality Test: ✅ PASSED\n\n🧪 INTEGRATION TEST 4: Business Logic Validation\n----------------------------------------\n🚀 Starting Complete E2E Data Pipeline\n==================================================\n\n📥 PHASE 1: DATA EXTRACTION\n==============================\n📥 STEP 1: Extracting Customer Data\n----------------------------------------\n🔄 Connecting to CRM system...\n🔄 Executing customer data query...\n🎲 Generating 300 realistic customers...\n✅ Successfully extracted 300 customer records\n📊 Sample extracted data:\n+-----------+----------+---------+--------------------+-------------------+--------------------+----------------+--------------------+-------------+-----------------+--------------+------------+-------------+\n|customer_id|first_name|last_name|               email|              phone|             address|            city|             country|date_of_birth|registration_date|account_status|credit_score|annual_income|\n+-----------+----------+---------+--------------------+-------------------+--------------------+----------------+--------------------+-------------+-----------------+--------------+------------+-------------+\n|  CUST00001|     Julie|    Ortiz|agriffin@example.org|  (887)731-0471x420|PSC 6723, Box 545...|  East Kaylastad|Syrian Arab Republic|   1993-10-10|       2023-12-19|        active|         702|        64330|\n|  CUST00002|     Maria|   Arnold|  xbrown@example.com|(521)846-2133x98737|8652 Guerrero Tra...|    Johnsonhaven|              Uganda|   1972-10-04|       2024-10-03|     suspended|         433|        54816|\n|  CUST00003|     Brett|    Smith|    llee@example.net|  940.741.2875x7356|16286 George Ligh...|Lake Jessicaside|             Senegal|   1957-04-30|       2024-08-08|      inactive|         438|        74707|\n+-----------+----------+---------+--------------------+-------------------+--------------------+----------------+--------------------+-------------+-----------------+--------------+------------+-------------+\nonly showing top 3 rows\n\n\n📥 STEP 2: Extracting Transaction Data\n----------------------------------------\n🔄 Connecting to transaction database...\n🔄 Executing transaction data query...\n🎲 Generating 1500 realistic transactions...\n✅ Successfully extracted 1500 transaction records\n📊 Sample extracted data:\n+--------------+-----------+----------------+----------------+-------+----------------+--------------------+--------+--------------+---------+\n|transaction_id|customer_id|transaction_date|transaction_time| amount|transaction_type|       merchant_name|category|payment_method|   status|\n+--------------+-----------+----------------+----------------+-------+----------------+--------------------+--------+--------------+---------+\n|   TXN00000001|  CUST00174|      2024-12-14|        18:31:52|1085.02|          refund|         Brown Group|  travel|    debit_card|completed|\n|   TXN00000002|  CUST00191|      2024-11-08|        22:06:30|1307.95|          refund|Mccarty, Gardner ...|  travel| bank_transfer|completed|\n|   TXN00000003|  CUST00163|      2024-09-26|        22:15:32| 331.61|          refund|Joseph, White and...|clothing|   credit_card|   failed|\n+--------------+-----------+----------------+----------------+-------+----------------+--------------------+--------+--------------+---------+\nonly showing top 3 rows\n\n\n⚙️ PHASE 2: DATA TRANSFORMATION\n==============================\n⚙️ STEP 1: Transforming Customer Data\n----------------------------------------\n✅ Transformed 300 customer records\n📊 Customer segmentation summary:\n+----------------+-----+\n|customer_segment|count|\n+----------------+-----+\n|         Premium|   30|\n|           Basic|   70|\n|            Risk|  144|\n|        Standard|   56|\n+----------------+-----+\n\n\n⚙️ STEP 2: Transforming Transaction Data\n----------------------------------------\n✅ Transformed 1500 transaction records\n📊 Transaction amount distribution:\n+-----------------+-----+\n|  amount_category|count|\n+-----------------+-----+\n|     Medium Value|  332|\n|       High Value| 1102|\n|Micro Transaction|    9|\n|        Low Value|   57|\n+-----------------+-----+\n\n\n⚙️ STEP 3: Creating Customer Analytics\n----------------------------------------\n✅ Created analytics for 300 customers\n📊 Customer tier distribution:\n+-------------+-----+\n|customer_tier|count|\n+-------------+-----+\n|     Platinum|   86|\n|       Silver|   58|\n|         Gold|   75|\n|       Bronze|   81|\n+-------------+-----+\n\n\n💾 PHASE 3: DATA LOADING\n==============================\n💾 Loading data to warehouse table: customers\n----------------------------------------\n🔍 Validating data quality...\n🔄 Connecting to data warehouse...\n🔄 Inserting records...\n🔄 Creating indexes...\n🔄 Committing transaction...\n✅ Successfully loaded 300 records to customers\n💾 Loading data to warehouse table: transactions\n----------------------------------------\n🔍 Validating data quality...\n🔄 Connecting to data warehouse...\n🔄 Inserting records...\n🔄 Creating indexes...\n🔄 Committing transaction...\n✅ Successfully loaded 1500 records to transactions\n💾 Loading data to analytics store\n----------------------------------------\n🔄 Connecting to analytics store...\n🔄 Loading customer analytics...\n🔄 Updating dashboards...\n✅ Successfully loaded 300 analytics records\n\n📊 PIPELINE EXECUTION SUMMARY\n========================================\n📥 Extraction: ✅ SUCCESS\n⚙️ Transformation: ✅ SUCCESS\n💾 Loading: ✅ SUCCESS\n🎯 Overall Pipeline: ✅ SUCCESS\n\n📊 Data Processed:\n  👥 Customers: 300\n  💳 Transactions: 1500\n  📈 Analytics Records: 300\n\n🏢 Business Logic Results:\n  premium_credit_logic: ✅ PASSED\n  platinum_value_logic: ✅ PASSED\n  risk_category_logic: ✅ PASSED\n  positive_spending: ✅ PASSED\n\n🎯 Business Logic Test: ✅ PASSED\n\n🧪 INTEGRATION TEST 5: Extraction Failure\n----------------------------------------\n✅ Successfully simulated extraction failure\n🎯 Extraction Failure Test: ✅ PASSED\n\n============================================================\n📊 INTEGRATION TEST SUITE SUMMARY\n============================================================\nsmall_dataset        : ✅ PASSED\nlarge_dataset        : ✅ PASSED\ndata_quality         : ✅ PASSED\nbusiness_logic       : ✅ PASSED\nextraction_failure   : ✅ PASSED\n\n📈 OVERALL RESULTS:\nTotal Tests: 5\nPassed: 5\nFailed: 0\nPass Rate: 100.0%\n\n🎉 ALL INTEGRATION TESTS PASSED!\n✅ Your E2E pipeline is robust and production-ready!\n"
     ]
    }
   ],
   "source": [
    "def test_e2e_pipeline_small_dataset():\n",
    "    \"\"\"\n",
    "    Integration Test 1: Small dataset (happy path)\n",
    "    \"\"\"\n",
    "    print(\"🧪 INTEGRATION TEST 1: Small Dataset\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results, analytics_data = run_complete_e2e_pipeline(num_customers=50, num_transactions=200)\n",
    "    \n",
    "    # Validate results\n",
    "    test_passed = (\n",
    "        results[\"extraction_success\"] and\n",
    "        results[\"transformation_success\"] and\n",
    "        results[\"loading_success\"] and\n",
    "        results[\"total_customers\"] == 50 and\n",
    "        results[\"total_transactions\"] == 200\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🎯 Small Dataset Test: {'✅ PASSED' if test_passed else '❌ FAILED'}\")\n",
    "    return test_passed\n",
    "\n",
    "def test_e2e_pipeline_large_dataset():\n",
    "    \"\"\"\n",
    "    Integration Test 2: Large dataset (performance test)\n",
    "    \"\"\"\n",
    "    print(\"\\n🧪 INTEGRATION TEST 2: Large Dataset\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results, analytics_data = run_complete_e2e_pipeline(num_customers=2000, num_transactions=10000)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    # Validate results and performance\n",
    "    test_passed = (\n",
    "        results[\"extraction_success\"] and\n",
    "        results[\"transformation_success\"] and\n",
    "        results[\"loading_success\"] and\n",
    "        execution_time < 120  # Should complete within 2 minutes\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n⏱️ Execution Time: {execution_time:.2f} seconds\")\n",
    "    print(f\"🎯 Large Dataset Test: {'✅ PASSED' if test_passed else '❌ FAILED'}\")\n",
    "    return test_passed\n",
    "\n",
    "def test_e2e_pipeline_data_quality():\n",
    "    \"\"\"\n",
    "    Integration Test 3: Data quality validation\n",
    "    \"\"\"\n",
    "    print(\"\\n🧪 INTEGRATION TEST 3: Data Quality Validation\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Run pipeline with moderate dataset\n",
    "    results, analytics_data = run_complete_e2e_pipeline(num_customers=500, num_transactions=2000)\n",
    "    \n",
    "    if analytics_data is None:\n",
    "        print(\"❌ No data to validate\")\n",
    "        return False\n",
    "    \n",
    "    # Data quality checks\n",
    "    quality_checks = {}\n",
    "    \n",
    "    # Check 1: No null customer IDs\n",
    "    null_customer_ids = analytics_data.filter(col(\"customer_id\").isNull()).count()\n",
    "    quality_checks[\"no_null_ids\"] = (null_customer_ids == 0)\n",
    "    \n",
    "    # Check 2: All customers have valid email format\n",
    "    invalid_emails = analytics_data.filter(~col(\"email_clean\").contains(\"@\")).count()\n",
    "    quality_checks[\"valid_emails\"] = (invalid_emails == 0)\n",
    "    \n",
    "    # Check 3: Credit scores in valid range\n",
    "    invalid_credit_scores = analytics_data.filter(\n",
    "        (col(\"credit_score\") < 300) | (col(\"credit_score\") > 850)\n",
    "    ).count()\n",
    "    quality_checks[\"valid_credit_scores\"] = (invalid_credit_scores == 0)\n",
    "    \n",
    "    # Check 4: Customer segments assigned\n",
    "    unassigned_segments = analytics_data.filter(col(\"customer_segment\").isNull()).count()\n",
    "    quality_checks[\"segments_assigned\"] = (unassigned_segments == 0)\n",
    "    \n",
    "    # Check 5: Age calculations reasonable\n",
    "    invalid_ages = analytics_data.filter((col(\"age\") < 18) | (col(\"age\") > 100)).count()\n",
    "    quality_checks[\"reasonable_ages\"] = (invalid_ages == 0)\n",
    "    \n",
    "    print(\"\\n🔍 Data Quality Results:\")\n",
    "    for check_name, passed in quality_checks.items():\n",
    "        status = \"✅ PASSED\" if passed else \"❌ FAILED\"\n",
    "        print(f\"  {check_name}: {status}\")\n",
    "    \n",
    "    all_quality_passed = all(quality_checks.values())\n",
    "    print(f\"\\n🎯 Data Quality Test: {'✅ PASSED' if all_quality_passed else '❌ FAILED'}\")\n",
    "    return all_quality_passed\n",
    "\n",
    "def test_e2e_pipeline_business_logic():\n",
    "    \"\"\"\n",
    "    Integration Test 4: Business logic validation\n",
    "    \"\"\"\n",
    "    print(\"\\n🧪 INTEGRATION TEST 4: Business Logic Validation\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Run pipeline\n",
    "    results, analytics_data = run_complete_e2e_pipeline(num_customers=300, num_transactions=1500)\n",
    "    \n",
    "    if analytics_data is None:\n",
    "        print(\"❌ No data to validate\")\n",
    "        return False\n",
    "    \n",
    "    # Business logic checks\n",
    "    business_checks = {}\n",
    "    \n",
    "    # Check 1: Premium customers have high credit scores\n",
    "    premium_customers = analytics_data.filter(col(\"customer_segment\") == \"Premium\")\n",
    "    low_credit_premium = premium_customers.filter(col(\"credit_score\") < 750).count()\n",
    "    business_checks[\"premium_credit_logic\"] = (low_credit_premium == 0)\n",
    "    \n",
    "    # Check 2: Customer tiers align with value scores\n",
    "    platinum_customers = analytics_data.filter(col(\"customer_tier\") == \"Platinum\")\n",
    "    low_value_platinum = platinum_customers.filter(col(\"customer_value_score\") < 1000).count()\n",
    "    business_checks[\"platinum_value_logic\"] = (low_value_platinum == 0)\n",
    "    \n",
    "    # Check 3: Risk categories align with credit scores\n",
    "    low_risk_customers = analytics_data.filter(col(\"risk_category\") == \"Low Risk\")\n",
    "    low_credit_low_risk = low_risk_customers.filter(col(\"credit_score\") < 750).count()\n",
    "    business_checks[\"risk_category_logic\"] = (low_credit_low_risk == 0)\n",
    "    \n",
    "    # Check 4: Total spent is non-negative\n",
    "    negative_spending = analytics_data.filter(col(\"total_spent\") < 0).count()\n",
    "    business_checks[\"positive_spending\"] = (negative_spending == 0)\n",
    "    \n",
    "    print(\"\\n🏢 Business Logic Results:\")\n",
    "    for check_name, passed in business_checks.items():\n",
    "        status = \"✅ PASSED\" if passed else \"❌ FAILED\"\n",
    "        print(f\"  {check_name}: {status}\")\n",
    "    \n",
    "    all_business_passed = all(business_checks.values())\n",
    "    print(f\"\\n🎯 Business Logic Test: {'✅ PASSED' if all_business_passed else '❌ FAILED'}\")\n",
    "    return all_business_passed\n",
    "\n",
    "def test_e2e_pipeline_with_extraction_failure():\n",
    "    \"\"\"\n",
    "    Integration Test 5: Extraction failure scenario\n",
    "    \"\"\"\n",
    "    print(\"\\n🧪 INTEGRATION TEST 5: Extraction Failure\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Simulate extraction failure by causing Faker to fail\n",
    "    try:\n",
    "        # This will fail because we're passing invalid parameters\n",
    "        fake.random_int(min=1000, max=10)  # min > max causes failure\n",
    "        test_passed = False\n",
    "    except:\n",
    "        print(\"✅ Successfully simulated extraction failure\")\n",
    "        test_passed = True\n",
    "    \n",
    "    print(f\"🎯 Extraction Failure Test: {'✅ PASSED' if test_passed else '❌ FAILED'}\")\n",
    "    return test_passed\n",
    "\n",
    "# ===============================================\n",
    "# PART 8: RUN COMPLETE INTEGRATION TEST SUITE\n",
    "# ===============================================\n",
    "\n",
    "print(\"\\n\\n🏆 PART 8: COMPLETE INTEGRATION TEST SUITE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def run_complete_integration_test_suite():\n",
    "    \"\"\"\n",
    "    Run all integration tests and provide comprehensive results\n",
    "    \"\"\"\n",
    "    print(\"🧪 Running Complete Integration Test Suite\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    test_results = {}\n",
    "    \n",
    "    # Run all integration tests\n",
    "    print(\"\\n🚀 Starting Integration Test Execution...\")\n",
    "    \n",
    "    test_results[\"small_dataset\"] = test_e2e_pipeline_small_dataset()\n",
    "    test_results[\"large_dataset\"] = test_e2e_pipeline_large_dataset()\n",
    "    test_results[\"data_quality\"] = test_e2e_pipeline_data_quality()\n",
    "    test_results[\"business_logic\"] = test_e2e_pipeline_business_logic()\n",
    "    test_results[\"extraction_failure\"] = test_e2e_pipeline_with_extraction_failure()\n",
    "    \n",
    "    # Calculate overall results\n",
    "    total_tests = len(test_results)\n",
    "    passed_tests = sum(test_results.values())\n",
    "    pass_rate = (passed_tests / total_tests) * 100\n",
    "    \n",
    "    # Print comprehensive summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"📊 INTEGRATION TEST SUITE SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for test_name, passed in test_results.items():\n",
    "        status = \"✅ PASSED\" if passed else \"❌ FAILED\"\n",
    "        print(f\"{test_name:20} : {status}\")\n",
    "    \n",
    "    print(f\"\\n📈 OVERALL RESULTS:\")\n",
    "    print(f\"Total Tests: {total_tests}\")\n",
    "    print(f\"Passed: {passed_tests}\")\n",
    "    print(f\"Failed: {total_tests - passed_tests}\")\n",
    "    print(f\"Pass Rate: {pass_rate:.1f}%\")\n",
    "    \n",
    "    if passed_tests == total_tests:\n",
    "        print(\"\\n🎉 ALL INTEGRATION TESTS PASSED!\")\n",
    "        print(\"✅ Your E2E pipeline is robust and production-ready!\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ {total_tests - passed_tests} test(s) failed\")\n",
    "        print(\"🔧 Pipeline needs improvements before production deployment\")\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "# Run the complete test suite\n",
    "final_test_results = run_complete_integration_test_suite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb9b9b40-98e9-4533-bd9a-3417e69dcff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Integration Testing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}